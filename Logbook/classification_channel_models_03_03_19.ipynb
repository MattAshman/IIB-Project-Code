{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03/03/2019\n",
    "\n",
    "- Train a seperate model for segments in each channel.\n",
    "- Investigate the performance of many classification models.\n",
    "- Investigate the effects of feature selection using recursive feature elimination.\n",
    "- Features used are still normalised with respect to typical response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules. Set settings. Import data.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import pywt\n",
    "import math\n",
    "from epdata_tools import epdata_main, get_ep_features, get_ep_feature_dict\n",
    "from IPython.display import HTML\n",
    "\n",
    "from Augmentation import data_augmentation\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.feature_selection import SelectFromModel, RFECV\n",
    "from sklearn import svm, naive_bayes, neighbors, gaussian_process\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from scipy.spatial.distance import euclidean\n",
    "from statsmodels.robust import mad\n",
    "from fastdtw import fastdtw\n",
    "from FeatureExtraction.feature_tools import detect_peaks\n",
    "from tsfresh.feature_extraction import feature_calculators\n",
    "from sklearn.utils import resample\n",
    "\n",
    "import xgboost\n",
    "\n",
    "from IPython.display import display, clear_output\n",
    "import pdb\n",
    "\n",
    "plt.style.use('default')\n",
    "\n",
    "X = pd.read_pickle('/Users/matthewashman/github/MasterProject2018/Data/X_all_channel_labels.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Format\n",
    "Each row corresponds to a recording of the patient's response to either an S1 or S2 pulse at a particular coupling interval (as identified by the 'Coupling Interval' column) in a particular channel (as identified by the 'Channel' column)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Channel</th>\n",
       "      <th>Coupling Interval</th>\n",
       "      <th>Data</th>\n",
       "      <th>Patient</th>\n",
       "      <th>S1/S2</th>\n",
       "      <th>Type</th>\n",
       "      <th>Label 1</th>\n",
       "      <th>Label 2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CS1-2</td>\n",
       "      <td>340</td>\n",
       "      <td>[-636, -617, -652, -560, -482, -415, -383, -46...</td>\n",
       "      <td>1</td>\n",
       "      <td>S2</td>\n",
       "      <td>af</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CS1-2</td>\n",
       "      <td>340</td>\n",
       "      <td>[-903.0, -873.0, -935.0, -941.0, -910.0, -845....</td>\n",
       "      <td>1</td>\n",
       "      <td>S1</td>\n",
       "      <td>af</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CS1-2</td>\n",
       "      <td>340</td>\n",
       "      <td>[-931.0, -896.0, -896.0, -906.0, -858.0, -839....</td>\n",
       "      <td>1</td>\n",
       "      <td>S1</td>\n",
       "      <td>af</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CS3-4</td>\n",
       "      <td>340</td>\n",
       "      <td>[472, 464, 491, 523, 553, 706, 1019, 1404, 164...</td>\n",
       "      <td>1</td>\n",
       "      <td>S2</td>\n",
       "      <td>af</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CS3-4</td>\n",
       "      <td>340</td>\n",
       "      <td>[298.0, 292.0, 303.0, 311.0, 299.0, 395.0, 451...</td>\n",
       "      <td>1</td>\n",
       "      <td>S1</td>\n",
       "      <td>af</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Channel Coupling Interval  \\\n",
       "0   CS1-2               340   \n",
       "1   CS1-2               340   \n",
       "2   CS1-2               340   \n",
       "3   CS3-4               340   \n",
       "4   CS3-4               340   \n",
       "\n",
       "                                                Data Patient S1/S2 Type  \\\n",
       "0  [-636, -617, -652, -560, -482, -415, -383, -46...       1    S2   af   \n",
       "1  [-903.0, -873.0, -935.0, -941.0, -910.0, -845....       1    S1   af   \n",
       "2  [-931.0, -896.0, -896.0, -906.0, -858.0, -839....       1    S1   af   \n",
       "3  [472, 464, 491, 523, 553, 706, 1019, 1404, 164...       1    S2   af   \n",
       "4  [298.0, 292.0, 303.0, 311.0, 299.0, 395.0, 451...       1    S1   af   \n",
       "\n",
       "  Label 1 Label 2  \n",
       "0       0       0  \n",
       "1     NaN     NaN  \n",
       "2     NaN     NaN  \n",
       "3       0       0  \n",
       "4     NaN     NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove bad files with bad labels\n",
    "X = X[~(X['Label 1']=='-1') & ~(X['Label 2']=='-1')]\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into S1 and S2 DataFrames. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_S1 = X.loc[X['S1/S2']=='S1']\n",
    "X_S2 = X.loc[X['S1/S2']=='S2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training/Test Split\n",
    "Divide the patients into training/test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform training test split on patients. i.e., 3 af_patients in the test set and 7 in training.\n",
    "af_patients = X[(X['Type']=='af') & (X['S1/S2']=='S2')]['Patient'].unique()\n",
    "at_patients = X[(X['Type']=='at') & (X['S1/S2']=='S2')]['Patient'].unique()\n",
    "avnrt_patients = X[(X['Type']=='avnrt') & (X['S1/S2']=='S2')]['Patient'].unique()\n",
    "avrt_patients = X[(X['Type']=='avrt') & (X['S1/S2']=='S2')]['Patient'].unique()\n",
    "ep_patients = X[(X['Type']=='ep') & (X['S1/S2']=='S2')]['Patient'].unique()\n",
    "\n",
    "random.shuffle(af_patients); random.shuffle(at_patients); random.shuffle(avrt_patients); random.shuffle(avnrt_patients); random.shuffle(ep_patients)\n",
    "\n",
    "train_af_patients, test_af_patients = train_test_split(af_patients, test_size=0.3)\n",
    "train_at_patients, test_at_patients = train_test_split(at_patients, test_size=0.3)\n",
    "train_avnrt_patients, test_avnrt_patients = train_test_split(avnrt_patients, test_size=0.3)\n",
    "train_avrt_patients, test_avrt_patients = train_test_split(avrt_patients, test_size=0.3)\n",
    "train_ep_patients, test_ep_patients = train_test_split(ep_patients, test_size=0.3)\n",
    "\n",
    "# Store trining and test patients in dictionaries\n",
    "training_patients = {}\n",
    "training_patients['af'] = train_af_patients\n",
    "training_patients['at'] = train_at_patients\n",
    "training_patients['avnrt'] = train_avnrt_patients\n",
    "training_patients['avrt'] = train_avrt_patients\n",
    "training_patients['ep'] = train_ep_patients\n",
    "\n",
    "test_patients = {}\n",
    "test_patients['af'] = test_af_patients\n",
    "test_patients['at'] = test_at_patients\n",
    "test_patients['avnrt'] = test_avnrt_patients\n",
    "test_patients['avrt'] = test_avrt_patients\n",
    "test_patients['ep'] = test_ep_patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.concat([X_S2[(X_S2['Type']=='af') & ([(x in train_af_patients) for x in X_S2['Patient'].values])],\n",
    "                     X_S2[(X_S2['Type']=='at') & ([(x in train_af_patients) for x in X_S2['Patient'].values])],\n",
    "                     X_S2[(X_S2['Type']=='avnrt') & ([(x in train_af_patients) for x in X_S2['Patient'].values])],\n",
    "                     X_S2[(X_S2['Type']=='avrt') & ([(x in train_af_patients) for x in X_S2['Patient'].values])],\n",
    "                     X_S2[(X_S2['Type']=='ep') & ([(x in train_af_patients) for x in X_S2['Patient'].values])]])\n",
    "\n",
    "X_test = pd.concat([X_S2[(X_S2['Type']=='af') & ([(x in test_af_patients) for x in X_S2['Patient'].values])],\n",
    "                     X_S2[(X_S2['Type']=='at') & ([(x in test_af_patients) for x in X_S2['Patient'].values])],\n",
    "                     X_S2[(X_S2['Type']=='avnrt') & ([(x in test_af_patients) for x in X_S2['Patient'].values])],\n",
    "                     X_S2[(X_S2['Type']=='avrt') & ([(x in test_af_patients) for x in X_S2['Patient'].values])],\n",
    "                     X_S2[(X_S2['Type']=='ep') & ([(x in test_af_patients) for x in X_S2['Patient'].values])]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Models\n",
    "Choice of either training a single model to predict fractionation given an input segment from any channel, or training three seperate models to predict fractionation in for segments of a particular channel. Let's start with a single model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0779816513761468"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Proportion of responses that are fractionated\n",
    "prop_fractionated = np.sum(np.float64(X_train['Label 1'].values))/X_train.shape[0]\n",
    "prop_fractionated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation\n",
    "As the dataset is very asymetric (many more non-fractionated examples), here I augmented more fractionated examples as described in my technical milestone report. I have found that this improves classification performance very significantly - especially in reducing the number of false negatives (i.e. fractionated responses being predicted as non-fractionated)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib qt \n",
    "# Create new augmented data for each S2 row of X_compact\n",
    "X_train['Augmented'] = 0\n",
    "# Use for storing augmented_rows in the form of dicts\n",
    "augmented_list = []\n",
    "for _, row in X_train[X_train['Label 1']=='1'].iterrows():\n",
    "    \n",
    "    augmented_data = data_augmentation.augment_fractionation(row['Data'], 7, False)\n",
    "    for data in augmented_data:\n",
    "        augmented_row = {}\n",
    "        augmented_row['Data'] = data\n",
    "        augmented_row['Channel'] = row['Channel']\n",
    "        augmented_row['Coupling Interval'] = row['Coupling Interval']\n",
    "        augmented_row['Label 1'] = row['Label 1']\n",
    "        augmented_row['Label 2'] = row['Label 2']\n",
    "        augmented_row['Patient'] = row['Patient']\n",
    "        augmented_row['S1/S2'] = row['S1/S2']\n",
    "        augmented_row['Type'] = row['Type']\n",
    "        augmented_row['Augmented'] = 1\n",
    "        augmented_list.append(augmented_row)\n",
    "    \n",
    "\n",
    "augmented_data = pd.DataFrame(augmented_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_aug = pd.concat([X_train, augmented_data], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_1 = X_train[X_train['Label 1']=='1']\n",
    "X_train_0 = X_train[X_train['Label 1']=='0']\n",
    "X_train_1_resampled = resample(X_train_1, \n",
    "                               replace=True,     # sample with replacement\n",
    "                               n_samples=X_train_0.shape[0],    # to match majority class\n",
    "                               random_state=123) # reproducible results\n",
    "X_train_resampled = pd.concat([X_train_1_resampled, X_train_0], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Channel</th>\n",
       "      <th>Coupling Interval</th>\n",
       "      <th>Data</th>\n",
       "      <th>Patient</th>\n",
       "      <th>S1/S2</th>\n",
       "      <th>Type</th>\n",
       "      <th>Label 1</th>\n",
       "      <th>Label 2</th>\n",
       "      <th>Augmented</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CS5-6</td>\n",
       "      <td>290</td>\n",
       "      <td>[-1257, -10464, -17582, -15528, -1689, 11195, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>S2</td>\n",
       "      <td>ep</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CS5-6</td>\n",
       "      <td>260</td>\n",
       "      <td>[-451, -908, -1594, -2297, -3218, -4019, -3170...</td>\n",
       "      <td>1</td>\n",
       "      <td>S2</td>\n",
       "      <td>af</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CS3-4</td>\n",
       "      <td>260</td>\n",
       "      <td>[-263, -470, -617, -598, -921, -1036, -983, -8...</td>\n",
       "      <td>8</td>\n",
       "      <td>S2</td>\n",
       "      <td>af</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CS3-4</td>\n",
       "      <td>270</td>\n",
       "      <td>[-239, -174, -192, -283, -267, -223, -229, -30...</td>\n",
       "      <td>9</td>\n",
       "      <td>S2</td>\n",
       "      <td>af</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CS5-6</td>\n",
       "      <td>260</td>\n",
       "      <td>[-1945, -1770, -1681, -1625, -1630, -1606, -14...</td>\n",
       "      <td>9</td>\n",
       "      <td>S2</td>\n",
       "      <td>af</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CS5-6</td>\n",
       "      <td>260</td>\n",
       "      <td>[-806, -587, -414, -170, 5, 220, 153, 176, -21...</td>\n",
       "      <td>5</td>\n",
       "      <td>S2</td>\n",
       "      <td>af</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>CS5-6</td>\n",
       "      <td>250</td>\n",
       "      <td>[-1263, -1085, -852, -768, -657, -416, -324, -...</td>\n",
       "      <td>5</td>\n",
       "      <td>S2</td>\n",
       "      <td>af</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>CS3-4</td>\n",
       "      <td>270</td>\n",
       "      <td>[80, 208, 704, 1104, 1728, 2352, 2560, 1712, -...</td>\n",
       "      <td>4</td>\n",
       "      <td>S2</td>\n",
       "      <td>avrt</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>CS1-2</td>\n",
       "      <td>300</td>\n",
       "      <td>[1013, 877, 620, 121, -569, -1556, -2997, -431...</td>\n",
       "      <td>8</td>\n",
       "      <td>S2</td>\n",
       "      <td>af</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>CS1-2</td>\n",
       "      <td>270</td>\n",
       "      <td>[-148, -151, -86, -81, -120, -180, -76, -72, -...</td>\n",
       "      <td>9</td>\n",
       "      <td>S2</td>\n",
       "      <td>af</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>CS5-6</td>\n",
       "      <td>250</td>\n",
       "      <td>[-1779, -1575, -1539, -1335, -1165, -1057, -88...</td>\n",
       "      <td>8</td>\n",
       "      <td>S2</td>\n",
       "      <td>af</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>CS1-2</td>\n",
       "      <td>290</td>\n",
       "      <td>[357, 455, 272, 636, 686, 945, 921, 762, -124,...</td>\n",
       "      <td>2</td>\n",
       "      <td>S2</td>\n",
       "      <td>ep</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>CS5-6</td>\n",
       "      <td>280</td>\n",
       "      <td>[-9471, -16298, -9463, 992, 4122, 2392, -2861,...</td>\n",
       "      <td>1</td>\n",
       "      <td>S2</td>\n",
       "      <td>ep</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>CS3-4</td>\n",
       "      <td>260</td>\n",
       "      <td>[-147, -113, -185, -31, -52, -10, 221, 434, 62...</td>\n",
       "      <td>4</td>\n",
       "      <td>S2</td>\n",
       "      <td>af</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>CS5-6</td>\n",
       "      <td>250</td>\n",
       "      <td>[-1779, -1575, -1539, -1335, -1165, -1057, -88...</td>\n",
       "      <td>8</td>\n",
       "      <td>S2</td>\n",
       "      <td>af</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>CS3-4</td>\n",
       "      <td>280</td>\n",
       "      <td>[276, 483, 473, 823, 686, 365, 211, -150, -793...</td>\n",
       "      <td>1</td>\n",
       "      <td>S2</td>\n",
       "      <td>ep</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>CS5-6</td>\n",
       "      <td>250</td>\n",
       "      <td>[-1779, -1575, -1539, -1335, -1165, -1057, -88...</td>\n",
       "      <td>8</td>\n",
       "      <td>S2</td>\n",
       "      <td>af</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>CS5-6</td>\n",
       "      <td>280</td>\n",
       "      <td>[-9471, -16298, -9463, 992, 4122, 2392, -2861,...</td>\n",
       "      <td>1</td>\n",
       "      <td>S2</td>\n",
       "      <td>ep</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>CS3-4</td>\n",
       "      <td>270</td>\n",
       "      <td>[-196, -445, -1549, -3263, -4376, -4017, -2749...</td>\n",
       "      <td>8</td>\n",
       "      <td>S2</td>\n",
       "      <td>af</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>CS5-6</td>\n",
       "      <td>250</td>\n",
       "      <td>[-1263, -1085, -852, -768, -657, -416, -324, -...</td>\n",
       "      <td>5</td>\n",
       "      <td>S2</td>\n",
       "      <td>af</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>CS1-2</td>\n",
       "      <td>270</td>\n",
       "      <td>[341, 396, 279, 324, 402, 581, 480, 507, 533, ...</td>\n",
       "      <td>5</td>\n",
       "      <td>S2</td>\n",
       "      <td>af</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>CS1-2</td>\n",
       "      <td>260</td>\n",
       "      <td>[-51, -76, -1, -150, -154, -111, -178, -32, -5...</td>\n",
       "      <td>9</td>\n",
       "      <td>S2</td>\n",
       "      <td>af</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>CS5-6</td>\n",
       "      <td>250</td>\n",
       "      <td>[-1779, -1575, -1539, -1335, -1165, -1057, -88...</td>\n",
       "      <td>8</td>\n",
       "      <td>S2</td>\n",
       "      <td>af</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>CS1-2</td>\n",
       "      <td>260</td>\n",
       "      <td>[76, 6, 67, 107, 333, 326, 304, 311, 300, 464,...</td>\n",
       "      <td>5</td>\n",
       "      <td>S2</td>\n",
       "      <td>af</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>CS5-6</td>\n",
       "      <td>250</td>\n",
       "      <td>[-383, -426, -591, -881, -1348, -1865, -2326, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>S2</td>\n",
       "      <td>af</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>CS1-2</td>\n",
       "      <td>290</td>\n",
       "      <td>[357, 455, 272, 636, 686, 945, 921, 762, -124,...</td>\n",
       "      <td>2</td>\n",
       "      <td>S2</td>\n",
       "      <td>ep</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>CS3-4</td>\n",
       "      <td>250</td>\n",
       "      <td>[-16, 79, 103, 101, 93, 21, 28, -26, -99, -128...</td>\n",
       "      <td>1</td>\n",
       "      <td>S2</td>\n",
       "      <td>af</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>CS5-6</td>\n",
       "      <td>260</td>\n",
       "      <td>[-451, -908, -1594, -2297, -3218, -4019, -3170...</td>\n",
       "      <td>1</td>\n",
       "      <td>S2</td>\n",
       "      <td>af</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>CS1-2</td>\n",
       "      <td>240</td>\n",
       "      <td>[-585, -543, -485, -389, -341, -421, -324, -27...</td>\n",
       "      <td>5</td>\n",
       "      <td>S2</td>\n",
       "      <td>af</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>CS1-2</td>\n",
       "      <td>320</td>\n",
       "      <td>[-253, -268, -183, -191, -135, -156, -150, -23...</td>\n",
       "      <td>1</td>\n",
       "      <td>S2</td>\n",
       "      <td>at</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1176</th>\n",
       "      <td>CS1-2</td>\n",
       "      <td>380</td>\n",
       "      <td>[128, 96, 288, 544, 576, 720, 832, 896, 1184, ...</td>\n",
       "      <td>8</td>\n",
       "      <td>S2</td>\n",
       "      <td>ep</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1177</th>\n",
       "      <td>CS3-4</td>\n",
       "      <td>380</td>\n",
       "      <td>[608, 832, 1152, 1392, 1760, 2176, 2992, 4624,...</td>\n",
       "      <td>8</td>\n",
       "      <td>S2</td>\n",
       "      <td>ep</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1178</th>\n",
       "      <td>CS5-6</td>\n",
       "      <td>380</td>\n",
       "      <td>[9472, 10112, -16896, -32768, -32768, -25280, ...</td>\n",
       "      <td>8</td>\n",
       "      <td>S2</td>\n",
       "      <td>ep</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1179</th>\n",
       "      <td>CS1-2</td>\n",
       "      <td>360</td>\n",
       "      <td>[240, 256, 272, 496, 624, 624, 848, 1008, 1216...</td>\n",
       "      <td>8</td>\n",
       "      <td>S2</td>\n",
       "      <td>ep</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1180</th>\n",
       "      <td>CS3-4</td>\n",
       "      <td>360</td>\n",
       "      <td>[896, 1136, 1376, 1696, 2048, 2656, 3808, 6192...</td>\n",
       "      <td>8</td>\n",
       "      <td>S2</td>\n",
       "      <td>ep</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1181</th>\n",
       "      <td>CS5-6</td>\n",
       "      <td>360</td>\n",
       "      <td>[10800, 2800, -31536, -32768, -32768, -6080, 2...</td>\n",
       "      <td>8</td>\n",
       "      <td>S2</td>\n",
       "      <td>ep</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1182</th>\n",
       "      <td>CS1-2</td>\n",
       "      <td>340</td>\n",
       "      <td>[256, 288, 464, 656, 800, 896, 1168, 1360, 169...</td>\n",
       "      <td>8</td>\n",
       "      <td>S2</td>\n",
       "      <td>ep</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1183</th>\n",
       "      <td>CS3-4</td>\n",
       "      <td>340</td>\n",
       "      <td>[1232, 1712, 2000, 2464, 3152, 4496, 7472, 135...</td>\n",
       "      <td>8</td>\n",
       "      <td>S2</td>\n",
       "      <td>ep</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1184</th>\n",
       "      <td>CS5-6</td>\n",
       "      <td>340</td>\n",
       "      <td>[-2016, -32768, -32768, -32768, -2464, 22256, ...</td>\n",
       "      <td>8</td>\n",
       "      <td>S2</td>\n",
       "      <td>ep</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1185</th>\n",
       "      <td>CS1-2</td>\n",
       "      <td>320</td>\n",
       "      <td>[304, 480, 464, 480, 688, 784, 1104, 1376, 132...</td>\n",
       "      <td>8</td>\n",
       "      <td>S2</td>\n",
       "      <td>ep</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1186</th>\n",
       "      <td>CS3-4</td>\n",
       "      <td>320</td>\n",
       "      <td>[1152, 1488, 1744, 2336, 3216, 5088, 9360, 152...</td>\n",
       "      <td>8</td>\n",
       "      <td>S2</td>\n",
       "      <td>ep</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1187</th>\n",
       "      <td>CS5-6</td>\n",
       "      <td>320</td>\n",
       "      <td>[-3968, -32768, -32768, -32768, 2224, 21792, 2...</td>\n",
       "      <td>8</td>\n",
       "      <td>S2</td>\n",
       "      <td>ep</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1188</th>\n",
       "      <td>CS1-2</td>\n",
       "      <td>300</td>\n",
       "      <td>[160, 272, 416, 672, 768, 976, 1296, 1376, 139...</td>\n",
       "      <td>8</td>\n",
       "      <td>S2</td>\n",
       "      <td>ep</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1189</th>\n",
       "      <td>CS3-4</td>\n",
       "      <td>300</td>\n",
       "      <td>[1152, 1584, 2080, 2720, 3632, 6224, 10784, 13...</td>\n",
       "      <td>8</td>\n",
       "      <td>S2</td>\n",
       "      <td>ep</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1190</th>\n",
       "      <td>CS5-6</td>\n",
       "      <td>300</td>\n",
       "      <td>[-26352, -32768, -32768, -9872, 22720, 32544, ...</td>\n",
       "      <td>8</td>\n",
       "      <td>S2</td>\n",
       "      <td>ep</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1191</th>\n",
       "      <td>CS1-2</td>\n",
       "      <td>290</td>\n",
       "      <td>[352, 464, 576, 704, 848, 1136, 1248, 1472, 13...</td>\n",
       "      <td>8</td>\n",
       "      <td>S2</td>\n",
       "      <td>ep</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1192</th>\n",
       "      <td>CS3-4</td>\n",
       "      <td>290</td>\n",
       "      <td>[1280, 1744, 2208, 3072, 4208, 6944, 12480, 14...</td>\n",
       "      <td>8</td>\n",
       "      <td>S2</td>\n",
       "      <td>ep</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1193</th>\n",
       "      <td>CS5-6</td>\n",
       "      <td>290</td>\n",
       "      <td>[-32768, -32768, -32768, 1840, 25008, 28064, 2...</td>\n",
       "      <td>8</td>\n",
       "      <td>S2</td>\n",
       "      <td>ep</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1194</th>\n",
       "      <td>CS1-2</td>\n",
       "      <td>280</td>\n",
       "      <td>[128, 112, 176, 320, 512, 752, 896, 1088, 1088...</td>\n",
       "      <td>8</td>\n",
       "      <td>S2</td>\n",
       "      <td>ep</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1195</th>\n",
       "      <td>CS3-4</td>\n",
       "      <td>280</td>\n",
       "      <td>[928, 1280, 1712, 2272, 3120, 4688, 7120, 1033...</td>\n",
       "      <td>8</td>\n",
       "      <td>S2</td>\n",
       "      <td>ep</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1196</th>\n",
       "      <td>CS5-6</td>\n",
       "      <td>280</td>\n",
       "      <td>[-12880, -32768, -32768, -32768, 15872, 32752,...</td>\n",
       "      <td>8</td>\n",
       "      <td>S2</td>\n",
       "      <td>ep</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1197</th>\n",
       "      <td>CS1-2</td>\n",
       "      <td>270</td>\n",
       "      <td>[-192, -64, 112, 240, 432, 432, 464, 496, 704,...</td>\n",
       "      <td>8</td>\n",
       "      <td>S2</td>\n",
       "      <td>ep</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1198</th>\n",
       "      <td>CS3-4</td>\n",
       "      <td>270</td>\n",
       "      <td>[320, 464, 1008, 1472, 1920, 2480, 3264, 4000,...</td>\n",
       "      <td>8</td>\n",
       "      <td>S2</td>\n",
       "      <td>ep</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1199</th>\n",
       "      <td>CS5-6</td>\n",
       "      <td>270</td>\n",
       "      <td>[10656, 4544, -28608, -32768, -32768, -32768, ...</td>\n",
       "      <td>8</td>\n",
       "      <td>S2</td>\n",
       "      <td>ep</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1200</th>\n",
       "      <td>CS1-2</td>\n",
       "      <td>260</td>\n",
       "      <td>[-192, 0, 80, 80, 144, 288, 256, 432, 480, 672...</td>\n",
       "      <td>8</td>\n",
       "      <td>S2</td>\n",
       "      <td>ep</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1201</th>\n",
       "      <td>CS3-4</td>\n",
       "      <td>260</td>\n",
       "      <td>[256, 368, 672, 896, 1344, 1776, 2208, 3008, 4...</td>\n",
       "      <td>8</td>\n",
       "      <td>S2</td>\n",
       "      <td>ep</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1202</th>\n",
       "      <td>CS5-6</td>\n",
       "      <td>260</td>\n",
       "      <td>[14320, 11552, -11168, -32768, -32768, -32768,...</td>\n",
       "      <td>8</td>\n",
       "      <td>S2</td>\n",
       "      <td>ep</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1203</th>\n",
       "      <td>CS1-2</td>\n",
       "      <td>250</td>\n",
       "      <td>[-96, -112, -128, -224, -240, -112, -128, 0, 6...</td>\n",
       "      <td>8</td>\n",
       "      <td>S2</td>\n",
       "      <td>ep</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1204</th>\n",
       "      <td>CS3-4</td>\n",
       "      <td>250</td>\n",
       "      <td>[-48, -16, 16, 272, 288, 496, 768, 976, 1168, ...</td>\n",
       "      <td>8</td>\n",
       "      <td>S2</td>\n",
       "      <td>ep</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1205</th>\n",
       "      <td>CS5-6</td>\n",
       "      <td>250</td>\n",
       "      <td>[1808, 3424, 5424, 8480, 9696, -736, -27808, -...</td>\n",
       "      <td>8</td>\n",
       "      <td>S2</td>\n",
       "      <td>ep</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1206 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Channel Coupling Interval  \\\n",
       "0      CS5-6               290   \n",
       "1      CS5-6               260   \n",
       "2      CS3-4               260   \n",
       "3      CS3-4               270   \n",
       "4      CS5-6               260   \n",
       "5      CS5-6               260   \n",
       "6      CS5-6               250   \n",
       "7      CS3-4               270   \n",
       "8      CS1-2               300   \n",
       "9      CS1-2               270   \n",
       "10     CS5-6               250   \n",
       "11     CS1-2               290   \n",
       "12     CS5-6               280   \n",
       "13     CS3-4               260   \n",
       "14     CS5-6               250   \n",
       "15     CS3-4               280   \n",
       "16     CS5-6               250   \n",
       "17     CS5-6               280   \n",
       "18     CS3-4               270   \n",
       "19     CS5-6               250   \n",
       "20     CS1-2               270   \n",
       "21     CS1-2               260   \n",
       "22     CS5-6               250   \n",
       "23     CS1-2               260   \n",
       "24     CS5-6               250   \n",
       "25     CS1-2               290   \n",
       "26     CS3-4               250   \n",
       "27     CS5-6               260   \n",
       "28     CS1-2               240   \n",
       "29     CS1-2               320   \n",
       "...      ...               ...   \n",
       "1176   CS1-2               380   \n",
       "1177   CS3-4               380   \n",
       "1178   CS5-6               380   \n",
       "1179   CS1-2               360   \n",
       "1180   CS3-4               360   \n",
       "1181   CS5-6               360   \n",
       "1182   CS1-2               340   \n",
       "1183   CS3-4               340   \n",
       "1184   CS5-6               340   \n",
       "1185   CS1-2               320   \n",
       "1186   CS3-4               320   \n",
       "1187   CS5-6               320   \n",
       "1188   CS1-2               300   \n",
       "1189   CS3-4               300   \n",
       "1190   CS5-6               300   \n",
       "1191   CS1-2               290   \n",
       "1192   CS3-4               290   \n",
       "1193   CS5-6               290   \n",
       "1194   CS1-2               280   \n",
       "1195   CS3-4               280   \n",
       "1196   CS5-6               280   \n",
       "1197   CS1-2               270   \n",
       "1198   CS3-4               270   \n",
       "1199   CS5-6               270   \n",
       "1200   CS1-2               260   \n",
       "1201   CS3-4               260   \n",
       "1202   CS5-6               260   \n",
       "1203   CS1-2               250   \n",
       "1204   CS3-4               250   \n",
       "1205   CS5-6               250   \n",
       "\n",
       "                                                   Data Patient S1/S2  Type  \\\n",
       "0     [-1257, -10464, -17582, -15528, -1689, 11195, ...       1    S2    ep   \n",
       "1     [-451, -908, -1594, -2297, -3218, -4019, -3170...       1    S2    af   \n",
       "2     [-263, -470, -617, -598, -921, -1036, -983, -8...       8    S2    af   \n",
       "3     [-239, -174, -192, -283, -267, -223, -229, -30...       9    S2    af   \n",
       "4     [-1945, -1770, -1681, -1625, -1630, -1606, -14...       9    S2    af   \n",
       "5     [-806, -587, -414, -170, 5, 220, 153, 176, -21...       5    S2    af   \n",
       "6     [-1263, -1085, -852, -768, -657, -416, -324, -...       5    S2    af   \n",
       "7     [80, 208, 704, 1104, 1728, 2352, 2560, 1712, -...       4    S2  avrt   \n",
       "8     [1013, 877, 620, 121, -569, -1556, -2997, -431...       8    S2    af   \n",
       "9     [-148, -151, -86, -81, -120, -180, -76, -72, -...       9    S2    af   \n",
       "10    [-1779, -1575, -1539, -1335, -1165, -1057, -88...       8    S2    af   \n",
       "11    [357, 455, 272, 636, 686, 945, 921, 762, -124,...       2    S2    ep   \n",
       "12    [-9471, -16298, -9463, 992, 4122, 2392, -2861,...       1    S2    ep   \n",
       "13    [-147, -113, -185, -31, -52, -10, 221, 434, 62...       4    S2    af   \n",
       "14    [-1779, -1575, -1539, -1335, -1165, -1057, -88...       8    S2    af   \n",
       "15    [276, 483, 473, 823, 686, 365, 211, -150, -793...       1    S2    ep   \n",
       "16    [-1779, -1575, -1539, -1335, -1165, -1057, -88...       8    S2    af   \n",
       "17    [-9471, -16298, -9463, 992, 4122, 2392, -2861,...       1    S2    ep   \n",
       "18    [-196, -445, -1549, -3263, -4376, -4017, -2749...       8    S2    af   \n",
       "19    [-1263, -1085, -852, -768, -657, -416, -324, -...       5    S2    af   \n",
       "20    [341, 396, 279, 324, 402, 581, 480, 507, 533, ...       5    S2    af   \n",
       "21    [-51, -76, -1, -150, -154, -111, -178, -32, -5...       9    S2    af   \n",
       "22    [-1779, -1575, -1539, -1335, -1165, -1057, -88...       8    S2    af   \n",
       "23    [76, 6, 67, 107, 333, 326, 304, 311, 300, 464,...       5    S2    af   \n",
       "24    [-383, -426, -591, -881, -1348, -1865, -2326, ...       1    S2    af   \n",
       "25    [357, 455, 272, 636, 686, 945, 921, 762, -124,...       2    S2    ep   \n",
       "26    [-16, 79, 103, 101, 93, 21, 28, -26, -99, -128...       1    S2    af   \n",
       "27    [-451, -908, -1594, -2297, -3218, -4019, -3170...       1    S2    af   \n",
       "28    [-585, -543, -485, -389, -341, -421, -324, -27...       5    S2    af   \n",
       "29    [-253, -268, -183, -191, -135, -156, -150, -23...       1    S2    at   \n",
       "...                                                 ...     ...   ...   ...   \n",
       "1176  [128, 96, 288, 544, 576, 720, 832, 896, 1184, ...       8    S2    ep   \n",
       "1177  [608, 832, 1152, 1392, 1760, 2176, 2992, 4624,...       8    S2    ep   \n",
       "1178  [9472, 10112, -16896, -32768, -32768, -25280, ...       8    S2    ep   \n",
       "1179  [240, 256, 272, 496, 624, 624, 848, 1008, 1216...       8    S2    ep   \n",
       "1180  [896, 1136, 1376, 1696, 2048, 2656, 3808, 6192...       8    S2    ep   \n",
       "1181  [10800, 2800, -31536, -32768, -32768, -6080, 2...       8    S2    ep   \n",
       "1182  [256, 288, 464, 656, 800, 896, 1168, 1360, 169...       8    S2    ep   \n",
       "1183  [1232, 1712, 2000, 2464, 3152, 4496, 7472, 135...       8    S2    ep   \n",
       "1184  [-2016, -32768, -32768, -32768, -2464, 22256, ...       8    S2    ep   \n",
       "1185  [304, 480, 464, 480, 688, 784, 1104, 1376, 132...       8    S2    ep   \n",
       "1186  [1152, 1488, 1744, 2336, 3216, 5088, 9360, 152...       8    S2    ep   \n",
       "1187  [-3968, -32768, -32768, -32768, 2224, 21792, 2...       8    S2    ep   \n",
       "1188  [160, 272, 416, 672, 768, 976, 1296, 1376, 139...       8    S2    ep   \n",
       "1189  [1152, 1584, 2080, 2720, 3632, 6224, 10784, 13...       8    S2    ep   \n",
       "1190  [-26352, -32768, -32768, -9872, 22720, 32544, ...       8    S2    ep   \n",
       "1191  [352, 464, 576, 704, 848, 1136, 1248, 1472, 13...       8    S2    ep   \n",
       "1192  [1280, 1744, 2208, 3072, 4208, 6944, 12480, 14...       8    S2    ep   \n",
       "1193  [-32768, -32768, -32768, 1840, 25008, 28064, 2...       8    S2    ep   \n",
       "1194  [128, 112, 176, 320, 512, 752, 896, 1088, 1088...       8    S2    ep   \n",
       "1195  [928, 1280, 1712, 2272, 3120, 4688, 7120, 1033...       8    S2    ep   \n",
       "1196  [-12880, -32768, -32768, -32768, 15872, 32752,...       8    S2    ep   \n",
       "1197  [-192, -64, 112, 240, 432, 432, 464, 496, 704,...       8    S2    ep   \n",
       "1198  [320, 464, 1008, 1472, 1920, 2480, 3264, 4000,...       8    S2    ep   \n",
       "1199  [10656, 4544, -28608, -32768, -32768, -32768, ...       8    S2    ep   \n",
       "1200  [-192, 0, 80, 80, 144, 288, 256, 432, 480, 672...       8    S2    ep   \n",
       "1201  [256, 368, 672, 896, 1344, 1776, 2208, 3008, 4...       8    S2    ep   \n",
       "1202  [14320, 11552, -11168, -32768, -32768, -32768,...       8    S2    ep   \n",
       "1203  [-96, -112, -128, -224, -240, -112, -128, 0, 6...       8    S2    ep   \n",
       "1204  [-48, -16, 16, 272, 288, 496, 768, 976, 1168, ...       8    S2    ep   \n",
       "1205  [1808, 3424, 5424, 8480, 9696, -736, -27808, -...       8    S2    ep   \n",
       "\n",
       "     Label 1 Label 2  Augmented  \n",
       "0          1       1          0  \n",
       "1          1       2          0  \n",
       "2          1       1          0  \n",
       "3          1       1          0  \n",
       "4          1       1          0  \n",
       "5          1       2          0  \n",
       "6          1       1          0  \n",
       "7          1       1          0  \n",
       "8          1       1          0  \n",
       "9          1       1          0  \n",
       "10         1       1          0  \n",
       "11         1       1          0  \n",
       "12         1       1          0  \n",
       "13         1       1          0  \n",
       "14         1       1          0  \n",
       "15         1       1          0  \n",
       "16         1       1          0  \n",
       "17         1       1          0  \n",
       "18         1       1          0  \n",
       "19         1       1          0  \n",
       "20         1       1          0  \n",
       "21         1       1          0  \n",
       "22         1       1          0  \n",
       "23         1       1          0  \n",
       "24         1       2          0  \n",
       "25         1       1          0  \n",
       "26         1       1          0  \n",
       "27         1       2          0  \n",
       "28         1       1          0  \n",
       "29         1       1          0  \n",
       "...      ...     ...        ...  \n",
       "1176       0       0          0  \n",
       "1177       0       0          0  \n",
       "1178       0       0          0  \n",
       "1179       0       0          0  \n",
       "1180       0       0          0  \n",
       "1181       0       0          0  \n",
       "1182       0       0          0  \n",
       "1183       0       0          0  \n",
       "1184       0       0          0  \n",
       "1185       0       0          0  \n",
       "1186       0       0          0  \n",
       "1187       0       0          0  \n",
       "1188       0       0          0  \n",
       "1189       0       0          0  \n",
       "1190       0       0          0  \n",
       "1191       0       0          0  \n",
       "1192       0       0          0  \n",
       "1193       0       0          0  \n",
       "1194       0       0          0  \n",
       "1195       0       0          0  \n",
       "1196       0       0          0  \n",
       "1197       0       0          0  \n",
       "1198       0       0          0  \n",
       "1199       0       0          0  \n",
       "1200       0       0          0  \n",
       "1201       0       0          0  \n",
       "1202       0       0          0  \n",
       "1203       0       0          0  \n",
       "1204       0       0          0  \n",
       "1205       0       0          0  \n",
       "\n",
       "[1206 rows x 9 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_resampled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "Here the feature vectors are extracted for each row S2 response (including the augmented responses). A reference feature vector is subtracted (as described - the first S1 response of the patient (i.e. in the file with the largest S1/S2 coupling interval) is extracted and the feature vector calculated. This is subtracted from all feature vectors corresponding to all other S2 responses for that patient)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Extracting Test Features: 100.0%'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train_aug_feature_list = []\n",
    "X_train_feature_list = []\n",
    "X_test_feature_list = []\n",
    "for i, row in X_train_aug.iterrows():\n",
    "    clear_output(wait=True)\n",
    "    display('Extracting Training Features: ' + str(round(100*i/X_train_aug.index[-1],3)) + '%')\n",
    "    \n",
    "    # Get typical response for this patient and channel\n",
    "    typical_response = X_S1[(X_S1['Type']==row['Type']) & \n",
    "                           (X_S1['Patient']==row['Patient']) &\n",
    "                           (X_S1['Channel']==row['Channel'])\n",
    "                           ].sort_values(by=['Coupling Interval'], ascending=False).iloc[0]\n",
    "    \n",
    "    typical_feature_dict = get_good_feature_dict(typical_response['Data'])\n",
    "    feature_dict = get_good_feature_dict(row['Data'])\n",
    "    \n",
    "    # Normalise by subtracting 'typical' feature values\n",
    "    for k, v in feature_dict.items():\n",
    "        feature_dict[k] = v - typical_feature_dict[k]\n",
    "        \n",
    "    # Fill in the other column values\n",
    "    for col, value in row.iteritems():\n",
    "        feature_dict[col] = value\n",
    "        \n",
    "    X_train_aug_feature_list.append(feature_dict)\n",
    "    \n",
    "for i, row in X_train.iterrows():\n",
    "    clear_output(wait=True)\n",
    "    display('Extracting Training Features: ' + str(round(100*i/X_train.index[-1],3)) + '%')\n",
    "    \n",
    "    # Get typical response for this patient and channel\n",
    "    typical_response = X_S1[(X_S1['Type']==row['Type']) & \n",
    "                           (X_S1['Patient']==row['Patient']) &\n",
    "                           (X_S1['Channel']==row['Channel'])\n",
    "                           ].sort_values(by=['Coupling Interval'], ascending=False).iloc[0]\n",
    "    \n",
    "    typical_feature_dict = get_good_feature_dict(typical_response['Data'])\n",
    "    feature_dict = get_good_feature_dict(row['Data'])\n",
    "    \n",
    "    # Normalise by subtracting 'typical' feature values\n",
    "    for k, v in feature_dict.items():\n",
    "        feature_dict[k] = v - typical_feature_dict[k]\n",
    "        \n",
    "    # Fill in the other column values\n",
    "    for col, value in row.iteritems():\n",
    "        feature_dict[col] = value\n",
    "        \n",
    "    X_train.append(feature_dict)\n",
    "    \n",
    "for i, row in X_test.iterrows():\n",
    "    clear_output(wait=True)\n",
    "    display('Extracting Test Features: ' + str(round(100*i/X_test.index[-1],3)) + '%')\n",
    "    \n",
    "    # Get typical response for this patient and channel\n",
    "    typical_response = X_S1[(X_S1['Type']==row['Type']) & \n",
    "                           (X_S1['Patient']==row['Patient']) &\n",
    "                           (X_S1['Channel']==row['Channel'])\n",
    "                           ].sort_values(by=['Coupling Interval'], ascending=False).iloc[0]\n",
    "    typical_feature_dict = get_good_feature_dict(typical_response['Data'])\n",
    "    feature_dict = get_good_feature_dict(row['Data'])\n",
    "    \n",
    "    # Normalise by subtracting 'typical' feature values\n",
    "    for k, v in feature_dict.items():\n",
    "        feature_dict[k] = v - typical_feature_dict[k]\n",
    "        \n",
    "    # Fill in the other column values\n",
    "    for col, value in row.iteritems():\n",
    "        feature_dict[col] = value\n",
    "        \n",
    "    X_test_feature_list.append(feature_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_features = pd.DataFrame(X_train_feature_list)\n",
    "X_test_features = pd.DataFrame(X_test_feature_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_training_cs12 = X_train_features[X_train_features['Channel']=='CS1-2'].drop(['Channel', 'Coupling Interval', 'Data', 'Label 1', 'Label 2', 'Patient', 'Type', 'S1/S2'], axis=1)\n",
    "y_training_cs12 = X_train_features[X_train_features['Channel']=='CS1-2']['Label 1']\n",
    "info_training_cs12 = X_train_features[X_train_features['Channel']=='CS1-2'][['Channel', 'Coupling Interval', 'Data', 'Label 1', 'Label 2', 'Patient', 'Type', 'S1/S2']]\n",
    "x_test_cs12 = X_test_features[X_train_features['Channel']=='CS1-2'].drop(['Channel', 'Coupling Interval', 'Data', 'Label 1', 'Label 2', 'Patient', 'Type', 'S1/S2'], axis=1)\n",
    "y_test_cs12 = X_test_features[X_train_features['Channel']=='CS1-2']['Label 1']\n",
    "info_test_cs12 = X_test_features[X_train_features['Channel']=='CS1-2'][['Channel', 'Coupling Interval', 'Data', 'Label 1', 'Label 2', 'Patient', 'Type', 'S1/S2']]\n",
    "\n",
    "x_training_cs34 = X_train_features[X_train_features['Channel']=='CS3-4'].drop(['Channel', 'Coupling Interval', 'Data', 'Label 1', 'Label 2', 'Patient', 'Type', 'S1/S2'], axis=1)\n",
    "y_training_cs34 = X_train_features[X_train_features['Channel']=='CS3-4']['Label 1']\n",
    "info_training_cs34 = X_train_features[X_train_features['Channel']=='CS3-4'][['Channel', 'Coupling Interval', 'Data', 'Label 1', 'Label 2', 'Patient', 'Type', 'S1/S2']]\n",
    "x_test_cs34 = X_test_features[X_train_features['Channel']=='CS3-4'].drop(['Channel', 'Coupling Interval', 'Data', 'Label 1', 'Label 2', 'Patient', 'Type', 'S1/S2'], axis=1)\n",
    "y_test_cs34 = X_test_features[X_train_features['Channel']=='CS3-4']['Label 1']\n",
    "info_test_cs34 = X_test_features[X_train_features['Channel']=='CS3-4'][['Channel', 'Coupling Interval', 'Data', 'Label 1', 'Label 2', 'Patient', 'Type', 'S1/S2']]\n",
    "\n",
    "x_training_cs56 = X_train_features[X_train_features['Channel']=='CS5-6'].drop(['Channel', 'Coupling Interval', 'Data', 'Label 1', 'Label 2', 'Patient', 'Type', 'S1/S2'], axis=1)\n",
    "y_training_cs56 = X_train_features[X_train_features['Channel']=='CS5-6']['Label 1']\n",
    "info_training_cs56 = X_train_features[X_train_features['Channel']=='CS5-6'][['Channel', 'Coupling Interval', 'Data', 'Label 1', 'Label 2', 'Patient', 'Type', 'S1/S2']]\n",
    "x_test_cs56 = X_test_features[X_train_features['Channel']=='CS5-6'].drop(['Channel', 'Coupling Interval', 'Data', 'Label 1', 'Label 2', 'Patient', 'Type', 'S1/S2'], axis=1)\n",
    "y_test_cs56 = X_test_features[X_train_features['Channel']=='CS5-6']['Label 1']\n",
    "info_test_cs56 = X_test_features[X_train_features['Channel']=='CS5-6'][['Channel', 'Coupling Interval', 'Data', 'Label 1', 'Label 2', 'Patient', 'Type', 'S1/S2']]\n",
    "\n",
    "\n",
    "x_test = X_test_features.drop(['Channel', 'Coupling Interval', 'Data', 'Label 1', 'Label 2', 'Patient', 'Type', 'S1/S2'], axis=1)\n",
    "y_test = X_test_features['Label 1']\n",
    "info_test = X_test_features[['Channel', 'Coupling Interval', 'Data', 'Label 1', 'Label 2', 'Patient', 'Type', 'S1/S2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_cm(cm, labels, hide_zeroes=False, hide_diagonal=False, hide_threshold=None):\n",
    "    \"\"\"pretty print for confusion matrixes\"\"\"\n",
    "    columnwidth = max([len(x) for x in labels] + [5])  # 5 is value length\n",
    "    empty_cell = \" \" * columnwidth\n",
    "    \n",
    "    # Begin CHANGES\n",
    "    fst_empty_cell = (columnwidth-3)//2 * \" \" + \"t/p\" + (columnwidth-3)//2 * \" \"\n",
    "    \n",
    "    if len(fst_empty_cell) < len(empty_cell):\n",
    "        fst_empty_cell = \" \" * (len(empty_cell) - len(fst_empty_cell)) + fst_empty_cell\n",
    "    # Print header\n",
    "    print(\"    \" + fst_empty_cell, end=\" \")\n",
    "    # End CHANGES\n",
    "    \n",
    "    for label in labels:\n",
    "        print(\"%{0}s\".format(columnwidth) % label, end=\" \")\n",
    "        \n",
    "    print()\n",
    "    # Print rows\n",
    "    for i, label1 in enumerate(labels):\n",
    "        print(\"    %{0}s\".format(columnwidth) % label1, end=\" \")\n",
    "        for j in range(len(labels)):\n",
    "            cell = \"%{0}.1f\".format(columnwidth) % cm[i, j]\n",
    "            if hide_zeroes:\n",
    "                cell = cell if float(cm[i, j]) != 0 else empty_cell\n",
    "            if hide_diagonal:\n",
    "                cell = cell if i != j else empty_cell\n",
    "            if hide_threshold:\n",
    "                cell = cell if cm[i, j] > hide_threshold else empty_cell\n",
    "            print(cell, end=\" \")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Channel Specific Model Using All Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CS1-2 Model\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Logistic Regression\n",
      "Cross validation score:\n",
      "[0.94594595 0.59722222 0.88888889]\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Linear SVC\n",
      "Cross validation score:\n",
      "[0.94594595 0.75       0.875     ]\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Naive Bayes\n",
      "Cross validation score:\n",
      "[0.94594595 0.83333333 0.88888889]\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "GP\n",
      "Cross validation score:\n",
      "[0.91891892 0.93055556 0.90277778]\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "XGBoost\n",
      "Cross validation score:\n",
      "[0.91891892 0.81944444 0.93055556]\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Random Forest\n",
      "Cross validation score:\n",
      "[0.91891892 0.86111111 0.93055556]\n",
      "\n",
      "\n",
      "CS3-4 Model\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Logistic Regression\n",
      "Cross validation score:\n",
      "[0.67567568 0.70833333 0.75      ]\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Linear SVC\n",
      "Cross validation score:\n",
      "[0.75675676 0.77777778 0.81944444]\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Naive Bayes\n",
      "Cross validation score:\n",
      "[0.93243243 0.84722222 0.95833333]\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "GP\n",
      "Cross validation score:\n",
      "[0.93243243 0.94444444 0.94444444]\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "XGBoost\n",
      "Cross validation score:\n",
      "[0.87837838 0.93055556 0.95833333]\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Random Forest\n",
      "Cross validation score:\n",
      "[0.93243243 0.93055556 0.94444444]\n",
      "\n",
      "\n",
      "CS5-6 Model\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Logistic Regression\n",
      "Cross validation score:\n",
      "[0.78378378 0.70833333 0.94444444]\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Linear SVC\n",
      "Cross validation score:\n",
      "[0.91891892 0.70833333 0.97222222]\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Naive Bayes\n",
      "Cross validation score:\n",
      "[0.90540541 0.84722222 0.95833333]\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "GP\n",
      "Cross validation score:\n",
      "[0.93243243 0.875      0.91666667]\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "XGBoost\n",
      "Cross validation score:\n",
      "[0.97297297 0.93055556 0.94444444]\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Random Forest\n",
      "Cross validation score:\n",
      "[0.95945946 0.95833333 0.95833333]\n"
     ]
    }
   ],
   "source": [
    "# Get cross validation scores on training data, following by test score.\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "models = (LogisticRegression(penalty='l2', C=0.1, random_state=0, solver='liblinear', class_weight=\"balanced\"),  \n",
    "          svm.LinearSVC(penalty='l2', C=0.1, dual=False, class_weight=\"balanced\"),\n",
    "          naive_bayes.GaussianNB(),  \n",
    "          gaussian_process.GaussianProcessClassifier(kernel=1.0*RBF(1)), \n",
    "          xgboost.XGBClassifier(),\n",
    "          RandomForestClassifier())\n",
    "model_names = ('Logistic Regression', 'Linear SVC', 'Naive Bayes', 'GP', 'XGBoost', 'Random Forest')\n",
    "\n",
    "print('CS1-2 Model')\n",
    "for clf, model_name in zip(models, model_names):\n",
    "    print('~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "    print(model_name)\n",
    "    print('Cross validation score:')\n",
    "    print(cross_val_score(clf, x_training_cs12, y_training_cs12.values, cv=3))\n",
    "    \n",
    "\n",
    "print('\\n\\nCS3-4 Model')    \n",
    "for clf, model_name in zip(models, model_names):\n",
    "    print('~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "    print(model_name)\n",
    "    print('Cross validation score:')\n",
    "    print(cross_val_score(clf, x_training_cs34, y_training_cs34.values, cv=3))\n",
    "    \n",
    "print('\\n\\nCS5-6 Model') \n",
    "for clf, model_name in zip(models, model_names):\n",
    "    print('~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "    print(model_name)\n",
    "    print('Cross validation score:')\n",
    "    print(cross_val_score(clf, x_training_cs56, y_training_cs56.values, cv=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results of Classifiers - Feature Selection Using Feature Importance\n",
    "Here features are selected using feature importance/recursive feature elimination. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get cross validation scores on training data, following by test score.\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "# models = (LogisticRegression(penalty='l2', C=1, random_state=0, solver='liblinear', class_weight=\"balanced\"), \n",
    "#           svm.SVC(class_weight=\"balanced\"), \n",
    "#           svm.LinearSVC(penalty='l2', C=1, dual=False, class_weight=\"balanced\"),\n",
    "#           naive_bayes.GaussianNB(),  \n",
    "#           gaussian_process.GaussianProcessClassifier(kernel=1.0*RBF(1)), \n",
    "#           xgboost.XGBClassifier(scale_pos_weight=(1/prop_fractionated)))\n",
    "# model_names = ('Logistic Regression', 'RBF SVC', 'Linear SVC', 'Naive Bayes', 'GP', 'XGBoost')\n",
    "\n",
    "# feature_names = x_training_cs12.columns\n",
    "\n",
    "# print('Original number of features: ' + str(x_training_cs12.shape[1]))\n",
    "\n",
    "# # Select according to feature importance. \n",
    "# # xgb = xgboost.XGBClassifier(scale_pos_weight=(1/prop_fractionated))\n",
    "# # linear_svc = svm.LinearSVC(penalty='l1', C=10, dual=False)\n",
    "# # log_regression = LogisticRegression(penalty='l1', C=10, random_state=0, solver='liblinear')\n",
    "\n",
    "\n",
    "# # sfm = RFECV(log_regression, step=1, cv=3)\n",
    "# # sfm.fit(x_training_cs12.values, y_training_cs12.values)\n",
    "# # x_training_sparse = sfm.transform(x_training_cs12.values)\n",
    "# x_training_sparse = x_training_cs12.values\n",
    "# print('Number of features selected: ' + str(x_training_sparse.shape[1]))\n",
    "# mask = sfm.get_support() #list of booleans\n",
    "# selected_features = [] # The list of your K best features\n",
    "\n",
    "# for bool, feature in zip(mask, feature_names):\n",
    "#     if bool:\n",
    "#         selected_features.append(feature)\n",
    "        \n",
    "# print(selected_features)\n",
    "\n",
    "# for clf, model_name in zip(models, model_names):\n",
    "#     print('~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "#     print(model_name)\n",
    "#     print('Cross validation score:')\n",
    "#     print(cross_val_score(clf, x_training_sparse, y_training_cs12.values, cv=3))\n",
    "\n",
    "# # print('Test Score:')\n",
    "# # X_test_sparse = sfm.transform(X_test.values)\n",
    "# # xgb.fit(X_training_sparse, y_training_cs12.values)\n",
    "# # print(xgb.score(X_test_sparse, y_test.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CS1-2 Model\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Logistic Regression\n",
      "Test data score:\n",
      "0.819047619047619\n",
      "           t/p       Not Fractionated     Fractionated \n",
      "    Not Fractionated             69.0             15.0 \n",
      "        Fractionated              4.0             17.0 \n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Linear SVC\n",
      "Test data score:\n",
      "0.8761904761904762\n",
      "           t/p       Not Fractionated     Fractionated \n",
      "    Not Fractionated             76.0              8.0 \n",
      "        Fractionated              5.0             16.0 \n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Naive Bayes\n",
      "Test data score:\n",
      "0.9047619047619048\n",
      "           t/p       Not Fractionated     Fractionated \n",
      "    Not Fractionated             81.0              3.0 \n",
      "        Fractionated              7.0             14.0 \n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "GP\n",
      "Test data score:\n",
      "0.8666666666666667\n",
      "           t/p       Not Fractionated     Fractionated \n",
      "    Not Fractionated             84.0              0.0 \n",
      "        Fractionated             14.0              7.0 \n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "XGBoost\n",
      "Test data score:\n",
      "0.8952380952380953\n",
      "           t/p       Not Fractionated     Fractionated \n",
      "    Not Fractionated             84.0              0.0 \n",
      "        Fractionated             11.0             10.0 \n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Random Forest\n",
      "Test data score:\n",
      "0.8761904761904762\n",
      "           t/p       Not Fractionated     Fractionated \n",
      "    Not Fractionated             84.0              0.0 \n",
      "        Fractionated             13.0              8.0 \n",
      "\n",
      "\n",
      "CS3-4 Model\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Logistic Regression\n",
      "Test data score:\n",
      "0.8571428571428571\n",
      "           t/p       Not Fractionated     Fractionated \n",
      "    Not Fractionated             79.0             13.0 \n",
      "        Fractionated              2.0             11.0 \n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Linear SVC\n",
      "Test data score:\n",
      "0.9047619047619048\n",
      "           t/p       Not Fractionated     Fractionated \n",
      "    Not Fractionated             84.0              8.0 \n",
      "        Fractionated              2.0             11.0 \n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Naive Bayes\n",
      "Test data score:\n",
      "0.9619047619047619\n",
      "           t/p       Not Fractionated     Fractionated \n",
      "    Not Fractionated             90.0              2.0 \n",
      "        Fractionated              2.0             11.0 \n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "GP\n",
      "Test data score:\n",
      "0.8761904761904762\n",
      "           t/p       Not Fractionated     Fractionated \n",
      "    Not Fractionated             92.0              0.0 \n",
      "        Fractionated             13.0              0.0 \n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "XGBoost\n",
      "Test data score:\n",
      "0.9333333333333333\n",
      "           t/p       Not Fractionated     Fractionated \n",
      "    Not Fractionated             92.0              0.0 \n",
      "        Fractionated              7.0              6.0 \n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Random Forest\n",
      "Test data score:\n",
      "0.9333333333333333\n",
      "           t/p       Not Fractionated     Fractionated \n",
      "    Not Fractionated             92.0              0.0 \n",
      "        Fractionated              7.0              6.0 \n",
      "\n",
      "\n",
      "CS5-6 Model\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Logistic Regression\n",
      "Test data score:\n",
      "0.8666666666666667\n",
      "           t/p       Not Fractionated     Fractionated \n",
      "    Not Fractionated             85.0             14.0 \n",
      "        Fractionated              0.0              6.0 \n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Linear SVC\n",
      "Test data score:\n",
      "0.8952380952380953\n",
      "           t/p       Not Fractionated     Fractionated \n",
      "    Not Fractionated             88.0             11.0 \n",
      "        Fractionated              0.0              6.0 \n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Naive Bayes\n",
      "Test data score:\n",
      "0.8476190476190476\n",
      "           t/p       Not Fractionated     Fractionated \n",
      "    Not Fractionated             83.0             16.0 \n",
      "        Fractionated              0.0              6.0 \n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "GP\n",
      "Test data score:\n",
      "0.9523809523809523\n",
      "           t/p       Not Fractionated     Fractionated \n",
      "    Not Fractionated             97.0              2.0 \n",
      "        Fractionated              3.0              3.0 \n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "XGBoost\n",
      "Test data score:\n",
      "0.9428571428571428\n",
      "           t/p       Not Fractionated     Fractionated \n",
      "    Not Fractionated             94.0              5.0 \n",
      "        Fractionated              1.0              5.0 \n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Random Forest\n",
      "Test data score:\n",
      "0.9619047619047619\n",
      "           t/p       Not Fractionated     Fractionated \n",
      "    Not Fractionated             96.0              3.0 \n",
      "        Fractionated              1.0              5.0 \n"
     ]
    }
   ],
   "source": [
    "# x_test_sparse = sfm.transform(x_test_cs12.values)\n",
    "\n",
    "print('CS1-2 Model')\n",
    "for clf, model_name in zip(models, model_names):\n",
    "    print('~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "    print(model_name)\n",
    "    clf.fit(x_training_cs12.values, y_training_cs12.values)\n",
    "    print('Test data score:')\n",
    "    print(clf.score(x_test_cs12.values, y_test_cs12.values))\n",
    "    predictions = clf.predict(x_test_cs12.values)\n",
    "    cm = confusion_matrix(y_test_cs12.values, predictions)\n",
    "    print_cm(cm, ['Not Fractionated','Fractionated'])\n",
    "    \n",
    "\n",
    "print('\\n\\nCS3-4 Model')    \n",
    "for clf, model_name in zip(models, model_names):\n",
    "    print('~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "    print(model_name)\n",
    "    clf.fit(x_training_cs34.values, y_training_cs34.values)\n",
    "    print('Test data score:')\n",
    "    print(clf.score(x_test_cs34.values, y_test_cs34.values))\n",
    "    predictions = clf.predict(x_test_cs34.values)\n",
    "    cm = confusion_matrix(y_test_cs34.values, predictions)\n",
    "    print_cm(cm, ['Not Fractionated','Fractionated'])\n",
    "    \n",
    "print('\\n\\nCS5-6 Model') \n",
    "for clf, model_name in zip(models, model_names):\n",
    "    print('~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "    print(model_name)\n",
    "    clf.fit(x_training_cs56.values, y_training_cs56.values)\n",
    "    print('Test data score:')\n",
    "    print(clf.score(x_test_cs56.values, y_test_cs56.values))\n",
    "    predictions = clf.predict(x_test_cs56.values)\n",
    "    cm = confusion_matrix(y_test_cs56.values, predictions)\n",
    "    print_cm(cm, ['Not Fractionated','Fractionated'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:    0.4s finished\n",
      "\n",
      "[2019-03-03 22:29:19] Features: 1/8 -- score: 0.8975904789776609[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed:    0.4s finished\n",
      "\n",
      "[2019-03-03 22:29:19] Features: 2/8 -- score: 0.911248902546093[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed:    0.4s finished\n",
      "\n",
      "[2019-03-03 22:29:20] Features: 3/8 -- score: 0.9281216466686177[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed:    0.3s finished\n",
      "\n",
      "[2019-03-03 22:29:20] Features: 4/8 -- score: 0.9220002926543751[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    0.2s finished\n",
      "\n",
      "[2019-03-03 22:29:20] Features: 5/8 -- score: 0.8985220954053265[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    0.2s finished\n",
      "\n",
      "[2019-03-03 22:29:21] Features: 6/8 -- score: 0.8622927031509122[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.2s finished\n",
      "\n",
      "[2019-03-03 22:29:21] Features: 7/8 -- score: 0.8738879133743049[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.1s finished\n",
      "\n",
      "[2019-03-03 22:29:21] Features: 8/8 -- score: 0.8535155106818847"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features: ('Approximate Entropy: m=3 r=0.7', 'Location of Maximum Energy: M=14', 'Number of Peaks: set_thresh=False', 'Width of Maximum Energy: M=14, width_thresh=0.2')\n",
      "0.9142857142857143\n",
      "           t/p       Not Fractionated     Fractionated \n",
      "    Not Fractionated             84.0              0.0 \n",
      "        Fractionated              9.0             12.0 \n"
     ]
    }
   ],
   "source": [
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "\n",
    "log_reg = LogisticRegression(penalty='l2', C=0.1, random_state=0, solver='liblinear', class_weight=\"balanced\")\n",
    "xgb = xgboost.XGBClassifier()\n",
    "rf = RandomForestClassifier()\n",
    "feature_names = x_training_cs12.columns\n",
    "\n",
    "sfs1 = SFS(rf, \n",
    "           k_features=(4,8), \n",
    "           forward=True, \n",
    "           floating=False, \n",
    "           scoring='roc_auc',\n",
    "           cv=3,\n",
    "           verbose=2)\n",
    "\n",
    "y_training_cs12_bi = [int(y) for y in y_training_cs12.values]\n",
    "sfs1 = sfs1.fit(x_training_cs12.values, y_training_cs12_bi, custom_feature_names=feature_names)\n",
    "print('Selected features:', sfs1.k_feature_names_)\n",
    "\n",
    "x_training_cs12_sfs = sfs1.transform(x_training_cs12.values)\n",
    "x_test_cs12_sfs = sfs1.transform(x_test_cs12.values)\n",
    "\n",
    "rf.fit(x_training_cs12_sfs, y_training_cs12.values)\n",
    "print(rf.score(x_test_cs12_sfs, y_test_cs12.values))\n",
    "\n",
    "predictions = rf.predict(x_test_cs12_sfs)\n",
    "cm = confusion_matrix(y_test_cs12.values, predictions)\n",
    "print_cm(cm, ['Not Fractionated','Fractionated'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:    0.4s finished\n",
      "\n",
      "[2019-03-03 22:27:37] Features: 1/8 -- score: 0.8350099460073884[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed:    0.4s finished\n",
      "\n",
      "[2019-03-03 22:27:37] Features: 2/8 -- score: 0.8791542341574311[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed:    0.4s finished\n",
      "\n",
      "[2019-03-03 22:27:38] Features: 3/8 -- score: 0.9106741972151179[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed:    0.4s finished\n",
      "\n",
      "[2019-03-03 22:27:38] Features: 4/8 -- score: 0.9041435777209434[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    0.3s finished\n",
      "\n",
      "[2019-03-03 22:27:38] Features: 5/8 -- score: 0.9164091361182155[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    0.2s finished\n",
      "\n",
      "[2019-03-03 22:27:38] Features: 6/8 -- score: 0.8753356777493607[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.2s finished\n",
      "\n",
      "[2019-03-03 22:27:39] Features: 7/8 -- score: 0.8861839300937766[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features: ('Approximate Entropy: m=3 r=0.7', 'Index Mass Quantile: q=0.6', 'Location of Maximum Energy: M=14', 'Percentage Fractionation: thresh=0.01', 'Width of Maximum Energy: M=14, width_thresh=0.2')\n",
      "0.8285714285714286\n",
      "           t/p       Not Fractionated     Fractionated \n",
      "    Not Fractionated             76.0             16.0 \n",
      "        Fractionated              2.0             11.0 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.1s finished\n",
      "\n",
      "[2019-03-03 22:27:39] Features: 8/8 -- score: 0.8630665672065927"
     ]
    }
   ],
   "source": [
    "sfs1 = SFS(rf, \n",
    "           k_features=(4,8), \n",
    "           forward=True, \n",
    "           floating=False, \n",
    "           scoring='roc_auc',\n",
    "           cv=3, \n",
    "           verbose=2)\n",
    "\n",
    "y_training_cs34_bi = [int(y) for y in y_training_cs34.values]\n",
    "sfs1 = sfs1.fit(x_training_cs34.values, y_training_cs34_bi, custom_feature_names=feature_names)\n",
    "print('Selected features:', sfs1.k_feature_names_)\n",
    "\n",
    "x_training_cs34_sfs = sfs1.transform(x_training_cs34.values)\n",
    "x_test_cs34_sfs = sfs1.transform(x_test_cs34.values)\n",
    "\n",
    "rf.fit(x_training_cs34_sfs, y_training_cs34.values)\n",
    "print(rf.score(x_test_cs34_sfs, y_test_cs34.values))\n",
    "\n",
    "predictions = rf.predict(x_test_cs34_sfs)\n",
    "cm = confusion_matrix(y_test_cs34.values, predictions)\n",
    "print_cm(cm, ['Not Fractionated','Fractionated'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:    0.4s finished\n",
      "\n",
      "[2019-03-03 22:30:26] Features: 1/8 -- score: 0.9751068376068376[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed:    0.4s finished\n",
      "\n",
      "[2019-03-03 22:30:26] Features: 2/8 -- score: 0.9472360972360973[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed:    0.4s finished\n",
      "\n",
      "[2019-03-03 22:30:27] Features: 3/8 -- score: 0.9868520368520368[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed:    0.4s finished\n",
      "\n",
      "[2019-03-03 22:30:27] Features: 4/8 -- score: 0.9817238317238317[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    0.2s finished\n",
      "\n",
      "[2019-03-03 22:30:27] Features: 5/8 -- score: 0.989264208014208[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    0.2s finished\n",
      "\n",
      "[2019-03-03 22:30:28] Features: 6/8 -- score: 0.9651272338772339[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.2s finished\n",
      "\n",
      "[2019-03-03 22:30:28] Features: 7/8 -- score: 0.957738788988789[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.1s finished\n",
      "\n",
      "[2019-03-03 22:30:28] Features: 8/8 -- score: 0.9733377733377734"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features: ('Approximate Entropy: m=3 r=0.7', 'Index Mass Quantile: q=0.6', 'Number of Peaks: set_thresh=False', 'Sample Entropy Around Max Energy: width=60 r=0.025', 'Width of Maximum Energy: M=14, width_thresh=0.2')\n",
      "           t/p       Not Fractionated     Fractionated \n",
      "    Not Fractionated             96.0              3.0 \n",
      "        Fractionated              1.0              5.0 \n"
     ]
    }
   ],
   "source": [
    "sfs1 = SFS(rf, \n",
    "           k_features=(4,8), \n",
    "           forward=True, \n",
    "           floating=False, \n",
    "           scoring='roc_auc',\n",
    "           cv=3,\n",
    "           verbose=2)\n",
    "\n",
    "y_training_cs56_bi = [int(y) for y in y_training_cs56.values]\n",
    "sfs1 = sfs1.fit(x_training_cs56.values, y_training_cs56_bi, custom_feature_names=feature_names)\n",
    "print('Selected features:', sfs1.k_feature_names_)\n",
    "\n",
    "x_training_cs56_sfs = sfs1.transform(x_training_cs56.values)\n",
    "x_test_cs56_sfs = sfs1.transform(x_test_cs56.values)\n",
    "\n",
    "rf.fit(x_training_cs56_sfs, y_training_cs56.values)\n",
    "rf.score(x_test_cs56_sfs, y_test_cs56.values)\n",
    "\n",
    "predictions = rf.predict(x_test_cs56_sfs)\n",
    "cm = confusion_matrix(y_test_cs56.values, predictions)\n",
    "print_cm(cm, ['Not Fractionated','Fractionated'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_good_feature_dict(x, col_prefix=''):\n",
    "    feature_dict = {}\n",
    "    height_thresh=0.1\n",
    "    \n",
    "    feature_dict[col_prefix + 'Maximum Absolute Value'] = max(abs(x))\n",
    "    \n",
    "    # Hand engineered features\n",
    "    x = x/max(abs(x))\n",
    "#     feature_dict[col_prefix + 'Conduction Delay: set_thresh=False'] = get_delay(x)\n",
    "    peaks = get_peaks(x, height_thresh)\n",
    "    feature_dict[col_prefix + 'Number of Peaks: set_thresh=False'] = len(peaks[0])\n",
    "    feature_dict[col_prefix + 'Percentage Fractionation: thresh=0.01'] = percentage_fractionation(x, peaks[0], thresh=0.01)\n",
    "    \n",
    "    # Denoise x for remaining features\n",
    "    x = denoise(x)\n",
    "    max_energy_idx = get_location_of_max_energy(x)\n",
    "    feature_dict[col_prefix + 'Location of Maximum Energy: M=14'] = max_energy_idx\n",
    "    feature_dict[col_prefix + 'Sample Entropy Around Max Energy: width=60 r=0.025'] = get_local_sample_entropy(x, max_energy_idx, 60, m=2, r=0.025)\n",
    "    feature_dict[col_prefix + 'Width of Maximum Energy: M=14, width_thresh=0.2'] = get_width_max_energy(x, M=14, width_thresh=0.2)\n",
    "    \n",
    "    # Temporal features\n",
    "    feature_dict[col_prefix + 'Approximate Entropy: m=3 r=0.7'] = feature_calculators.approximate_entropy(x, 3, 0.7)\n",
    "    imq = feature_calculators.index_mass_quantile(x, [{'q': 0.6}])\n",
    "    feature_dict[col_prefix + 'Index Mass Quantile: q=0.6'] = imq[0][1]\n",
    "    feature_dict[col_prefix + 'Ratio Beyond 1xSTD'] = feature_calculators.ratio_beyond_r_sigma(x, 1)\n",
    "    \n",
    "    # Spectral features\n",
    "#     feature_dict[col_prefix + 'Power Spectral Entropy'] = get_pse(x)\n",
    "    \n",
    "    return feature_dict\n",
    "    \n",
    "\n",
    "def get_hand_engineered_feature_dict(x, thresh_cd=None, set_thresh_cd=False, thresh_peaks=None, set_thresh_peaks=False, show_peaks=False, col_prefix = ''):\n",
    "    feature_dict = {}\n",
    "    sf = max(abs(x))\n",
    "    x = x/max(abs(x))\n",
    "\n",
    "    # Hand engineered features\n",
    "    if set_thresh_cd:\n",
    "        thresh_cd = thresh_cd/sf\n",
    "        feature_dict[col_prefix + 'Conduction Delay: set_thresh=True'] = get_delay(x, thresh_cd, set_thresh_cd)\n",
    "        feature_dict[col_prefix + 'Conduction Delay: set_thresh=False'] = get_delay(x)\n",
    "    else:\n",
    "        feature_dict[col_prefix + 'Conduction Delay: set_thresh=False'] = get_delay(x)\n",
    "    \n",
    "    height_thresh=0.1\n",
    "    if set_thresh_peaks:\n",
    "        thresh_peaks = thresh_peaks/sf\n",
    "        peaks = get_peaks(x, height_thresh, thresh_peaks, set_thresh_peaks, plot=False)\n",
    "        feature_dict[col_prefix + 'Number of Peaks: set_thresh=True'] = len(peaks[0])\n",
    "        peaks = get_peaks(x, height_thresh)\n",
    "        feature_dict[col_prefix + 'Number of Peaks: set_thresh=False'] = len(peaks[0])\n",
    "    else:\n",
    "        peaks = get_peaks(x, height_thresh)\n",
    "        feature_dict[col_prefix + 'Number of Peaks: set_thresh=False'] = len(peaks[0])\n",
    "    \n",
    "    peaks = get_peaks(x, height_thresh)\n",
    "    feature_dict[col_prefix + 'Percentage Fractionation: thresh=0.01'] = percentage_fractionation(x, peaks[0], thresh=0.01)\n",
    "    \n",
    "    # Denoise x for remaining features\n",
    "    x = denoise(x)\n",
    "    \n",
    "    max_energy_idx = get_location_of_max_energy(x)\n",
    "    feature_dict[col_prefix + 'Location of Maximum Energy: M=14'] = max_energy_idx\n",
    "    feature_dict[col_prefix + 'Sample Entropy Around Max Energy: width=60 r=0.025'] = get_local_sample_entropy(x, max_energy_idx, 60, m=2, r=0.025)\n",
    "    feature_dict[col_prefix + 'Energy Around Max Energy'] = get_local_energy(x, max_energy_idx, 60)\n",
    "    min_idx = np.argmin(x)\n",
    "    max_idx = np.argmax(x)\n",
    "    feature_dict[col_prefix + 'Peaks Between Min and Max'] = len([i for i in peaks[0] if ((i > min_idx) & (i < max_idx))])\n",
    "    feature_dict[col_prefix + 'Width of Maximum Energy: M=14, width_thresh=0.4'] = get_width_max_energy(x, M=14, width_thresh=0.4)\n",
    "    feature_dict[col_prefix + 'Width of Maximum Energy: M=14, width_thresh=0.2'] = get_width_max_energy(x, M=14, width_thresh=0.2)\n",
    "\n",
    "    return feature_dict\n",
    "\n",
    "def get_spectral_feature_dict(x, col_prefix = ''):\n",
    "    feature_dict = {}\n",
    "    # Denoise and normalise x for remaining features\n",
    "    x = denoise(x)\n",
    "    x = x/max(abs(x))\n",
    "    \n",
    "    feature_dict[col_prefix + 'Power Spectral Entropy'] = get_pse(x)\n",
    "    feature_dict[col_prefix + 'Spectral Centroid'] = get_spectral_centroid(x)\n",
    "    max_energy_idx = get_location_of_max_energy(x)\n",
    "    feature_dict[col_prefix + 'Power Spectral Entropy Around Maximum Energy: width=30'] = get_local_pse(x, max_energy_idx, width=30)\n",
    "    feature_dict[col_prefix + 'Spectral Centroid Around Maximum Energy: width=30'] = get_local_spectral_centroid(x, max_energy_idx, width=30)\n",
    "    feature_dict[col_prefix + 'Power Spectral Entropy Around Maximum Energy: width=60'] = get_local_pse(x, max_energy_idx, width=60)\n",
    "    feature_dict[col_prefix + 'Spectral Centroid Around Maximum Energy: width=60'] = get_local_spectral_centroid(x, max_energy_idx, width=60)\n",
    "    \n",
    "    return feature_dict\n",
    "    \n",
    "def get_temporal_feature_dict(x, col_prefix = ''):\n",
    "\n",
    "    feature_dict = {}\n",
    "    feature_dict[col_prefix + 'Maximum Absolute Value'] = np.max(abs(x))\n",
    "    \n",
    "    # Denoise and normalise x for remaining features\n",
    "    x = denoise(x)\n",
    "    x = x/max(abs(x))\n",
    "\n",
    "\n",
    "    erbc = feature_calculators.energy_ratio_by_chunks(x, [{'num_segments':10, 'segment_focus':3}, {'num_segments':10, 'segment_focus':2}])\n",
    "    feature_dict[col_prefix + 'Energy Ratio by Chunks: num_segments=10 segment_focus=2'] = erbc[1][1]\n",
    "    feature_dict[col_prefix + 'Energy Ratio by Chunks: num_segments=10 segment_focus=3'] = erbc[0][1]\n",
    "    feature_dict[col_prefix + 'Approximate Entropy: m=3 r=0.7'] = feature_calculators.approximate_entropy(x, 3, 0.7)\n",
    "    feature_dict[col_prefix + 'Ratio Beyond 5xSTD'] = feature_calculators.ratio_beyond_r_sigma(x, 5)\n",
    "    feature_dict[col_prefix + 'Ratio Beyond 4xSTD'] = feature_calculators.ratio_beyond_r_sigma(x, 4)\n",
    "    feature_dict[col_prefix + 'Ratio Beyond 3xSTD'] = feature_calculators.ratio_beyond_r_sigma(x, 3)\n",
    "    feature_dict[col_prefix + 'Ratio Beyond 2xSTD'] = feature_calculators.ratio_beyond_r_sigma(x, 2)\n",
    "    feature_dict[col_prefix + 'Ratio Beyond 1xSTD'] = feature_calculators.ratio_beyond_r_sigma(x, 1)\n",
    "    # A fraction q of the mass lies to the left of i. (Alternative to conduction delay?)\n",
    "    imq = feature_calculators.index_mass_quantile(x, [{'q': 0.6}, {'q': 0.4}])\n",
    "    feature_dict[col_prefix + 'Index Mass Quantile: q=0.6'] = imq[0][1]\n",
    "    feature_dict[col_prefix + 'Index Mass Quantile: q=0.4'] = imq[1][1]\n",
    "    \n",
    "\n",
    "    return feature_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A shitty conduction delay detector\n",
    "def get_delay(x, amp_thresh=None, set_thresh=False):\n",
    "    if (set_thresh==True):\n",
    "        if any(abs(x)>amp_thresh):\n",
    "            return np.argmax(abs(x)>amp_thresh)\n",
    "        else:\n",
    "            return len(x)\n",
    "    else:    \n",
    "        return np.argmax(abs(x)>(max(abs(x))/2))\n",
    "    \n",
    "def denoise(x):\n",
    "    # Obtain Daubechies N=6 wavelet coefficients\n",
    "    waveletCoefs = pywt.wavedec(x, 'db7', mode='per')\n",
    "\n",
    "    # Throw away coefficients corresponding to noise\n",
    "    sigma = mad(waveletCoefs[-1])\n",
    "    uThresh = 1*sigma*np.sqrt(2*np.log(len(x)))\n",
    "    denoised = waveletCoefs[:]\n",
    "    denoised[1:] = (pywt._thresholding.hard(i, value=uThresh) for i in denoised[1:])\n",
    "\n",
    "    # Reconstruct the original signal\n",
    "    xDenoised = pywt.waverec(denoised, 'db7', mode='per')\n",
    "\n",
    "    return xDenoised\n",
    "\n",
    "def get_peaks(x, height_thresh, scale_amp=None, set_scale=False, plot = False):\n",
    "    x = np.array(x)\n",
    "    \n",
    "    # Get height_thresh\n",
    "    if set_scale:\n",
    "        height_thresh = height_thresh*scale_amp\n",
    "    else:\n",
    "        height_thresh = height_thresh*max(abs(x))\n",
    "    \n",
    "    # Denoise x\n",
    "    xdn = denoise(x)\n",
    "\n",
    "    # Detect peaks using detect_peaks\n",
    "    pos_peak_idx = detect_peaks(xdn, mph=height_thresh, threshold = 0)\n",
    "    neg_peak_idx = detect_peaks((-xdn), mph=height_thresh, threshold = 0)\n",
    "    peak_idx = np.concatenate([pos_peak_idx, neg_peak_idx])\n",
    "    peak_idx = np.sort(peak_idx)\n",
    "    # Edge indeces aren't detected\n",
    "    peak_idx = peak_idx[(peak_idx != 0) & (peak_idx != (len(xdn)-1))]\n",
    "\n",
    "    new_peak_idx = []\n",
    "    peak_amp = []\n",
    "    if (len(peak_idx) > 0):\n",
    "        new_peak_idx.append(peak_idx[0])\n",
    "        mp_thresh = 0.2*max(abs(x))\n",
    "        for i in range(len(peak_idx)-1):\n",
    "            idx = peak_idx[i]\n",
    "            idx_next = peak_idx[i+1]\n",
    "            mid_point = int((idx_next+idx)/2)\n",
    "            if (max([abs(x[idx_next]-x[mid_point]), abs(x[idx]-x[mid_point])]) > mp_thresh):\n",
    "                new_peak_idx.append(idx_next)\n",
    "\n",
    "        peak_idx = np.array(new_peak_idx)\n",
    "        peak_amp = x[peak_idx]\n",
    "\n",
    "    if plot == True:\n",
    "        fig, [ax1] = plt.subplots(nrows=1, ncols=1, sharex=True, figsize=(8,8))\n",
    "        ax1.plot(x, 'b' , xdn, 'r--', peak_idx, peak_amp, 'kx')\n",
    "        #plt.title(fileName)\n",
    "        ax1.set_xlabel('Sample')\n",
    "        ax1.set_ylabel('Normalised amplitude')\n",
    "        ax1.legend(['Original segment', 'Denoised segment', 'Detected peaks'])\n",
    "\n",
    "        plt.draw()\n",
    "        plt.waitforbuttonpress(0) # this will wait for indefinite time\n",
    "        plt.close(fig)\n",
    "\n",
    "\n",
    "    return peak_idx, peak_amp\n",
    "\n",
    "def sample_entropy(U, m, r):\n",
    "\n",
    "    def _maxdist(x_i, x_j):\n",
    "        result = max([abs(ua-va) for ua, va in zip(x_i, x_j)])\n",
    "        return result\n",
    "\n",
    "    def _phi(m):\n",
    "        x = np.zeros([N,m-1])\n",
    "        for i in range(N-m+1):\n",
    "            x[i,:] = U[i:i+m-1]\n",
    "\n",
    "        C = 0\n",
    "        for i in range(len(x)):\n",
    "            for j in range(len(x)):\n",
    "                if i != j:\n",
    "                    if _maxdist(x[i,:], x[j,:]) <= r:\n",
    "                        C = C + 1\n",
    "\n",
    "        return C\n",
    "\n",
    "    U = U/max(abs(U))\n",
    "    N = len(U)\n",
    "\n",
    "    return -np.log(_phi(m+1)/_phi(m))\n",
    "\n",
    "def percentage_fractionation(x, peak_idxs, thresh=0.01, sr=1000):\n",
    "    # Get peak indexes and amplitude\n",
    "    peak_idx_diffs = np.diff(peak_idxs)\n",
    "    frac_time = 0\n",
    "    frac_time = np.sum(peak_idx_diffs[peak_idx_diffs < thresh*sr])\n",
    "    prcnt_frac = (frac_time/len(x))*100\n",
    "    return prcnt_frac\n",
    "\n",
    "def get_local_sample_entropy(x, centre_idx, width, m=2, r=0.05):\n",
    "    # Ensure width is odd\n",
    "    if ((width%2) == 0):\n",
    "        width += 1\n",
    "        \n",
    "    if (centre_idx < (width-1)/2):\n",
    "        return sample_entropy(x[:width+1], m, r)\n",
    "    elif (centre_idx > (len(x)-1-(width-1)/2)):\n",
    "        return sample_entropy(x[len(x)-1-width:], m, r)\n",
    "    else:\n",
    "        return sample_entropy(x[int(centre_idx-(width-1)/2):int(centre_idx+(width+1)/2)], m, r)\n",
    "    \n",
    "def get_location_of_max_energy(x, M=14):\n",
    "    v = np.ones(M)\n",
    "    x_ = np.convolve(abs(x), v)\n",
    "    return (np.argmax(x_) + math.floor(M/2))\n",
    "        \n",
    "def get_local_peaks(x, centre_idx, width=25, height_thresh=0.1):\n",
    "    if ((width%2) == 0):\n",
    "        width += 1\n",
    "        \n",
    "    if (centre_idx < (width-1)/2):\n",
    "        return get_peaks(x[:width+1], height_thresh)\n",
    "    elif (centre_idx > (len(x)-1-(width-1)/2)):\n",
    "        return get_peaks(x[len(x)-1-width:], height_thresh)\n",
    "    else:\n",
    "        return get_peaks(x[int(centre_idx-(width-1)/2):int(centre_idx+(width+1)/2)], height_thresh)\n",
    "    \n",
    "def get_pse(x):\n",
    "    x_fft = np.fft.rfft(x)\n",
    "    x_P = (1/len(x_fft))*np.absolute(x_fft)**2\n",
    "    x_p = x_P/sum(x_P)\n",
    "    pse = np.sum([(-p*np.log2(p)) for p in x_p])\n",
    "    return pse\n",
    "\n",
    "def get_local_pse(x, centre_idx, width=50):\n",
    "    if ((width%2) == 0):\n",
    "        width += 1\n",
    "        \n",
    "    if (centre_idx < (width-1)/2):\n",
    "        return get_pse(x[:width+1])\n",
    "    elif (centre_idx > (len(x)-1-(width-1)/2)):\n",
    "        return get_pse(x[len(x)-1-width:])\n",
    "    else:\n",
    "        return get_pse(x[int(centre_idx-(width-1)/2):int(centre_idx+(width+1)/2)])\n",
    "    \n",
    "def get_spectral_centroid(x):\n",
    "    x_fft = np.fft.rfft(x)\n",
    "    x_spectrum = np.absolute(x_fft)\n",
    "    normalized_spectrum = x_spectrum/sum(x_spectrum)\n",
    "    normalized_frequencies = np.arange(0, len(x_spectrum), 1)\n",
    "    return sum(normalized_frequencies * normalized_spectrum)\n",
    "\n",
    "def get_local_spectral_centroid(x, centre_idx, width=50):\n",
    "    if ((width%2) == 0):\n",
    "        width += 1\n",
    "        \n",
    "    if (centre_idx < (width-1)/2):\n",
    "        return get_spectral_centroid(x[:width+1])\n",
    "    elif (centre_idx > (len(x)-1-(width-1)/2)):\n",
    "        return get_spectral_centroid(x[len(x)-1-width:])\n",
    "    else:\n",
    "        return get_spectral_centroid(x[int(centre_idx-(width-1)/2):int(centre_idx+(width+1)/2)])\n",
    "    \n",
    "def get_local_energy(x, centre_idx, width=60):\n",
    "    if ((width%2) == 0):\n",
    "        width += 1\n",
    "        \n",
    "    if (centre_idx < (width-1)/2):\n",
    "        return np.sum(x[:width+1]**2)\n",
    "    elif (centre_idx > (len(x)-1-(width-1)/2)):\n",
    "        return np.sum(x[len(x)-1-width:]**2)\n",
    "    else:\n",
    "        return np.sum(x[int(centre_idx-(width-1)/2):int(centre_idx+(width+1)/2)]**2)\n",
    "    \n",
    "def get_width_max_energy(x, M=14, width_thresh=0.2):\n",
    "    v = np.ones(M)\n",
    "    x_ = np.convolve(abs(x), v)\n",
    "    if any(x_[np.argmax(x_):] < width_thresh*np.max(x_)):\n",
    "        end_idx = np.argmax(x_) + np.argmax(x_[np.argmax(x_):] < width_thresh*np.max(x_))\n",
    "    else:\n",
    "        end_idx = len(x_)-1\n",
    "    if any(x_[np.argmax(x_)::-1] < width_thresh*np.max(x_)):  \n",
    "        start_idx = np.argmax(x_) - np.argmax(x_[np.argmax(x_)::-1] < width_thresh*np.max(x_))\n",
    "    else:\n",
    "        start_idx = 0\n",
    "    return (end_idx - start_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
