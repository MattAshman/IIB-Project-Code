{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 26/02/2019\n",
    "\n",
    "- Investigating the performance of a single model trained on segments from all electrodes.\n",
    "- Investigate performance of different machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules. Set settings. Import data.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from epdata_tools import epdata_main, get_ep_features, get_ep_feature_dict\n",
    "from IPython.display import HTML\n",
    "\n",
    "from Augmentation import data_augmentation\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.feature_selection import SelectFromModel, RFECV\n",
    "from sklearn import svm, naive_bayes, neighbors, gaussian_process\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from scipy.spatial.distance import euclidean\n",
    "from fastdtw import fastdtw\n",
    "\n",
    "import xgboost\n",
    "\n",
    "from IPython.display import display, clear_output\n",
    "import pdb\n",
    "\n",
    "plt.style.use('default')\n",
    "\n",
    "X = pd.read_pickle('/Users/matthewashman/github/MasterProject2018/Data/X_all_channel_labels.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Format\n",
    "Each row corresponds to a recording of the patient's response to either an S1 or S2 pulse at a particular coupling interval (as identified by the 'Coupling Interval' column) in a particular channel (as identified by the 'Channel' column)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Channel</th>\n",
       "      <th>Coupling Interval</th>\n",
       "      <th>Data</th>\n",
       "      <th>Patient</th>\n",
       "      <th>S1/S2</th>\n",
       "      <th>Type</th>\n",
       "      <th>Label 1</th>\n",
       "      <th>Label 2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CS1-2</td>\n",
       "      <td>340</td>\n",
       "      <td>[-636, -617, -652, -560, -482, -415, -383, -46...</td>\n",
       "      <td>1</td>\n",
       "      <td>S2</td>\n",
       "      <td>af</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CS1-2</td>\n",
       "      <td>340</td>\n",
       "      <td>[-903.0, -873.0, -935.0, -941.0, -910.0, -845....</td>\n",
       "      <td>1</td>\n",
       "      <td>S1</td>\n",
       "      <td>af</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CS1-2</td>\n",
       "      <td>340</td>\n",
       "      <td>[-931.0, -896.0, -896.0, -906.0, -858.0, -839....</td>\n",
       "      <td>1</td>\n",
       "      <td>S1</td>\n",
       "      <td>af</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CS3-4</td>\n",
       "      <td>340</td>\n",
       "      <td>[472, 464, 491, 523, 553, 706, 1019, 1404, 164...</td>\n",
       "      <td>1</td>\n",
       "      <td>S2</td>\n",
       "      <td>af</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CS3-4</td>\n",
       "      <td>340</td>\n",
       "      <td>[298.0, 292.0, 303.0, 311.0, 299.0, 395.0, 451...</td>\n",
       "      <td>1</td>\n",
       "      <td>S1</td>\n",
       "      <td>af</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Channel Coupling Interval  \\\n",
       "0   CS1-2               340   \n",
       "1   CS1-2               340   \n",
       "2   CS1-2               340   \n",
       "3   CS3-4               340   \n",
       "4   CS3-4               340   \n",
       "\n",
       "                                                Data Patient S1/S2 Type  \\\n",
       "0  [-636, -617, -652, -560, -482, -415, -383, -46...       1    S2   af   \n",
       "1  [-903.0, -873.0, -935.0, -941.0, -910.0, -845....       1    S1   af   \n",
       "2  [-931.0, -896.0, -896.0, -906.0, -858.0, -839....       1    S1   af   \n",
       "3  [472, 464, 491, 523, 553, 706, 1019, 1404, 164...       1    S2   af   \n",
       "4  [298.0, 292.0, 303.0, 311.0, 299.0, 395.0, 451...       1    S1   af   \n",
       "\n",
       "  Label 1 Label 2  \n",
       "0       0       0  \n",
       "1     NaN     NaN  \n",
       "2     NaN     NaN  \n",
       "3       0       0  \n",
       "4     NaN     NaN  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove bad files with bad labels\n",
    "X = X[~(X['Label 1']=='-1') & ~(X['Label 2']=='-1')]\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into S1 and S2 DataFrames. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_S1 = X.loc[X['S1/S2']=='S1']\n",
    "X_S2 = X.loc[X['S1/S2']=='S2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training/Test Split\n",
    "Divide the patients into training/test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform training test split on patients. i.e., 3 af_patients in the test set and 7 in training.\n",
    "af_patients = X[(X['Type']=='af') & (X['S1/S2']=='S2')]['Patient'].unique()\n",
    "at_patients = X[(X['Type']=='at') & (X['S1/S2']=='S2')]['Patient'].unique()\n",
    "avnrt_patients = X[(X['Type']=='avnrt') & (X['S1/S2']=='S2')]['Patient'].unique()\n",
    "avrt_patients = X[(X['Type']=='avrt') & (X['S1/S2']=='S2')]['Patient'].unique()\n",
    "ep_patients = X[(X['Type']=='ep') & (X['S1/S2']=='S2')]['Patient'].unique()\n",
    "\n",
    "random.shuffle(af_patients); random.shuffle(at_patients); random.shuffle(avrt_patients); random.shuffle(avnrt_patients); random.shuffle(ep_patients)\n",
    "\n",
    "train_af_patients, test_af_patients = train_test_split(af_patients, test_size=0.3)\n",
    "train_at_patients, test_at_patients = train_test_split(at_patients, test_size=0.3)\n",
    "train_avnrt_patients, test_avnrt_patients = train_test_split(avnrt_patients, test_size=0.3)\n",
    "train_avrt_patients, test_avrt_patients = train_test_split(avrt_patients, test_size=0.3)\n",
    "train_ep_patients, test_ep_patients = train_test_split(ep_patients, test_size=0.3)\n",
    "\n",
    "# Store trining and test patients in dictionaries\n",
    "training_patients = {}\n",
    "training_patients['af'] = train_af_patients\n",
    "training_patients['at'] = train_at_patients\n",
    "training_patients['avnrt'] = train_avnrt_patients\n",
    "training_patients['avrt'] = train_avrt_patients\n",
    "training_patients['ep'] = train_ep_patients\n",
    "\n",
    "test_patients = {}\n",
    "test_patients['af'] = test_af_patients\n",
    "test_patients['at'] = test_at_patients\n",
    "test_patients['avnrt'] = test_avnrt_patients\n",
    "test_patients['avrt'] = test_avrt_patients\n",
    "test_patients['ep'] = test_ep_patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.concat([X_S2[(X_S2['Type']=='af') & ([(x in train_af_patients) for x in X_S2['Patient'].values])],\n",
    "                     X_S2[(X_S2['Type']=='at') & ([(x in train_af_patients) for x in X_S2['Patient'].values])],\n",
    "                     X_S2[(X_S2['Type']=='avnrt') & ([(x in train_af_patients) for x in X_S2['Patient'].values])],\n",
    "                     X_S2[(X_S2['Type']=='avrt') & ([(x in train_af_patients) for x in X_S2['Patient'].values])],\n",
    "                     X_S2[(X_S2['Type']=='ep') & ([(x in train_af_patients) for x in X_S2['Patient'].values])]])\n",
    "\n",
    "X_test = pd.concat([X_S2[(X_S2['Type']=='af') & ([(x in test_af_patients) for x in X_S2['Patient'].values])],\n",
    "                     X_S2[(X_S2['Type']=='at') & ([(x in test_af_patients) for x in X_S2['Patient'].values])],\n",
    "                     X_S2[(X_S2['Type']=='avnrt') & ([(x in test_af_patients) for x in X_S2['Patient'].values])],\n",
    "                     X_S2[(X_S2['Type']=='avrt') & ([(x in test_af_patients) for x in X_S2['Patient'].values])],\n",
    "                     X_S2[(X_S2['Type']=='ep') & ([(x in test_af_patients) for x in X_S2['Patient'].values])]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Models\n",
    "Choice of either training a single model to predict fractionation given an input segment from any channel, or training three seperate models to predict fractionation in for segments of a particular channel. Let's start with a single model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08571428571428572"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Proportion of responses that are fractionated\n",
    "prop_fractionated = np.sum(np.float64(X_train['Label 1'].values))/X_train.shape[0]\n",
    "prop_fractionated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation\n",
    "As the dataset is very asymetric (many more non-fractionated examples), here I augmented more fractionated examples as described in my technical milestone report. I have found that this improves classification performance very significantly - especially in reducing the number of false negatives (i.e. fractionated responses being predicted as non-fractionated)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib qt \n",
    "# Create new augmented data for each S2 row of X_compact\n",
    "X_train['Augmented'] = 0\n",
    "# Use for storing augmented_rows in the form of dicts\n",
    "augmented_list = []\n",
    "for _, row in X_train[X_train['Label 1']=='1'].iterrows():\n",
    "    \n",
    "    augmented_data = data_augmentation.augment_fractionation(row['Data'], 7, False)\n",
    "    for data in augmented_data:\n",
    "        augmented_row = {}\n",
    "        augmented_row['Data'] = data\n",
    "        augmented_row['Channel'] = row['Channel']\n",
    "        augmented_row['Coupling Interval'] = row['Coupling Interval']\n",
    "        augmented_row['Label 1'] = row['Label 1']\n",
    "        augmented_row['Label 2'] = row['Label 2']\n",
    "        augmented_row['Patient'] = row['Patient']\n",
    "        augmented_row['S1/S2'] = row['S1/S2']\n",
    "        augmented_row['Type'] = row['Type']\n",
    "        augmented_row['Augmented'] = 1\n",
    "        augmented_list.append(augmented_row)\n",
    "    \n",
    "\n",
    "augmented_data = pd.DataFrame(augmented_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = pd.concat([X_train, augmented_data], ignore_index=True)\n",
    "X_train = pd.concat([X_train], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "Here the feature vectors are extracted for each row S2 response (including the augmented responses). A reference feature vector is subtracted (as described - the first S1 response of the patient (i.e. in the file with the largest S1/S2 coupling interval) is extracted and the feature vector calculated. This is subtracted from all feature vectors corresponding to all other S2 responses for that patient)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Extracting Test Features: 100.0%'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train_feature_list = []\n",
    "X_test_feature_list = []\n",
    "for i, row in X_train.iterrows():\n",
    "    clear_output(wait=True)\n",
    "    display('Extracting Training Features: ' + str(round(100*i/X_train.index[-1],3)) + '%')\n",
    "    \n",
    "    # Get typical response for this patient and channel\n",
    "    typical_response = X_S1[(X_S1['Type']==row['Type']) & \n",
    "                           (X_S1['Patient']==row['Patient']) &\n",
    "                           (X_S1['Channel']==row['Channel'])\n",
    "                           ].sort_values(by=['Coupling Interval'], ascending=False).iloc[0]\n",
    "    typical_feature_dict = get_ep_feature_dict(typical_response['Data'])\n",
    "    feature_dict = get_ep_feature_dict(row['Data'])\n",
    "    \n",
    "    # Normalise by subtracting 'typical' feature values\n",
    "    for k, v in feature_dict.items():\n",
    "        feature_dict[k] = v - typical_feature_dict[k]\n",
    "        \n",
    "    # Add DTW distance\n",
    "#     distance, path = fastdtw(row['Data']/max(abs(row['Data'])), typical_response['Data']/max(abs(typical_response['Data'])), dist=euclidean)\n",
    "#     feature_dict['DTW'] = distance\n",
    "        \n",
    "    # Fill in the other column values\n",
    "    for col, value in row.iteritems():\n",
    "        feature_dict[col] = value\n",
    "        \n",
    "    X_train_feature_list.append(feature_dict)\n",
    "    \n",
    "for i, row in X_test.iterrows():\n",
    "    clear_output(wait=True)\n",
    "    display('Extracting Test Features: ' + str(round(100*i/X_test.index[-1],3)) + '%')\n",
    "    \n",
    "    # Get typical response for this patient and channel\n",
    "    typical_response = X_S1[(X_S1['Type']==row['Type']) & \n",
    "                           (X_S1['Patient']==row['Patient']) &\n",
    "                           (X_S1['Channel']==row['Channel'])\n",
    "                           ].sort_values(by=['Coupling Interval'], ascending=False).iloc[0]\n",
    "    typical_feature_dict = get_ep_feature_dict(typical_response['Data'])\n",
    "    feature_dict = get_ep_feature_dict(row['Data'])\n",
    "    \n",
    "    # Normalise by subtracting 'typical' feature values\n",
    "    for k, v in feature_dict.items():\n",
    "        feature_dict[k] = v - typical_feature_dict[k]\n",
    "        \n",
    "    # Add DTW distance\n",
    "#     distance, path = fastdtw(row['Data']/max(abs(row['Data'])), typical_response['Data']/max(abs(typical_response['Data'])), dist=euclidean)\n",
    "#     feature_dict['DTW'] = distance\n",
    "        \n",
    "    # Fill in the other column values\n",
    "    for col, value in row.iteritems():\n",
    "        feature_dict[col] = value\n",
    "        \n",
    "    X_test_feature_list.append(feature_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_features = pd.DataFrame(X_train_feature_list)\n",
    "X_test_features = pd.DataFrame(X_test_feature_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_training = X_train_features.drop(['Augmented', 'Channel', 'Coupling Interval', 'Data', 'Label 1', 'Label 2', 'Patient', 'Type', 'S1/S2'], axis=1)\n",
    "y_training = X_train_features['Label 1']\n",
    "info_training = X_train_features[['Augmented', 'Channel', 'Coupling Interval', 'Data', 'Label 1', 'Label 2', 'Patient', 'Type', 'S1/S2']]\n",
    "\n",
    "x_test = X_test_features.drop(['Channel', 'Coupling Interval', 'Data', 'Label 1', 'Label 2', 'Patient', 'Type', 'S1/S2'], axis=1)\n",
    "y_test = X_test_features['Label 1']\n",
    "info_test = X_test_features[['Channel', 'Coupling Interval', 'Data', 'Label 1', 'Label 2', 'Patient', 'Type', 'S1/S2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Approximate Entropy: m=2 r=0.7</th>\n",
       "      <th>Conduction Delay</th>\n",
       "      <th>Number of Peaks</th>\n",
       "      <th>Percentage Fractionation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.188661</td>\n",
       "      <td>44</td>\n",
       "      <td>5</td>\n",
       "      <td>13.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.265399</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>25.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.147361</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>16.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0.060564</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0.150661</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>9.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0.139740</td>\n",
       "      <td>19</td>\n",
       "      <td>7</td>\n",
       "      <td>18.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>0.092506</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>16.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>0.146783</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>11.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>0.156046</td>\n",
       "      <td>30</td>\n",
       "      <td>6</td>\n",
       "      <td>21.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>0.236426</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>15.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>0.118556</td>\n",
       "      <td>41</td>\n",
       "      <td>5</td>\n",
       "      <td>17.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>0.225062</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>18.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>0.130659</td>\n",
       "      <td>54</td>\n",
       "      <td>5</td>\n",
       "      <td>18.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>0.205429</td>\n",
       "      <td>21</td>\n",
       "      <td>6</td>\n",
       "      <td>18.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>0.195988</td>\n",
       "      <td>57</td>\n",
       "      <td>8</td>\n",
       "      <td>18.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>0.188888</td>\n",
       "      <td>36</td>\n",
       "      <td>6</td>\n",
       "      <td>12.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>0.110335</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>0.206138</td>\n",
       "      <td>32</td>\n",
       "      <td>6</td>\n",
       "      <td>17.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>0.107762</td>\n",
       "      <td>18</td>\n",
       "      <td>5</td>\n",
       "      <td>13.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>0.169369</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>17.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>0.180150</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>7.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>0.181776</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>5.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>0.172950</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>0.106949</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>16.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>0.197321</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>0.178037</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>14.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>0.208397</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>26.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>0.188709</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>0.215483</td>\n",
       "      <td>29</td>\n",
       "      <td>5</td>\n",
       "      <td>9.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>0.205593</td>\n",
       "      <td>22</td>\n",
       "      <td>6</td>\n",
       "      <td>14.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>0.178670</td>\n",
       "      <td>36</td>\n",
       "      <td>7</td>\n",
       "      <td>12.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>0.134803</td>\n",
       "      <td>33</td>\n",
       "      <td>4</td>\n",
       "      <td>12.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>0.107180</td>\n",
       "      <td>25</td>\n",
       "      <td>5</td>\n",
       "      <td>9.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>0.263119</td>\n",
       "      <td>35</td>\n",
       "      <td>7</td>\n",
       "      <td>23.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>0.159768</td>\n",
       "      <td>33</td>\n",
       "      <td>4</td>\n",
       "      <td>13.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>0.139712</td>\n",
       "      <td>24</td>\n",
       "      <td>6</td>\n",
       "      <td>17.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>0.176980</td>\n",
       "      <td>19</td>\n",
       "      <td>5</td>\n",
       "      <td>18.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>0.074642</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>12.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>0.206701</td>\n",
       "      <td>20</td>\n",
       "      <td>6</td>\n",
       "      <td>8.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>0.154928</td>\n",
       "      <td>19</td>\n",
       "      <td>7</td>\n",
       "      <td>14.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>0.125688</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>14.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>0.240463</td>\n",
       "      <td>22</td>\n",
       "      <td>8</td>\n",
       "      <td>22.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>0.243056</td>\n",
       "      <td>16</td>\n",
       "      <td>9</td>\n",
       "      <td>20.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>0.197682</td>\n",
       "      <td>27</td>\n",
       "      <td>7</td>\n",
       "      <td>12.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>0.153212</td>\n",
       "      <td>21</td>\n",
       "      <td>4</td>\n",
       "      <td>2.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>0.087896</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>0.082989</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>11.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418</th>\n",
       "      <td>0.043526</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>6.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421</th>\n",
       "      <td>0.068250</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422</th>\n",
       "      <td>0.040497</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>8.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>533</th>\n",
       "      <td>0.133678</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>13.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>594</th>\n",
       "      <td>0.108492</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595</th>\n",
       "      <td>0.178303</td>\n",
       "      <td>21</td>\n",
       "      <td>5</td>\n",
       "      <td>6.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>0.120622</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>11.333333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Approximate Entropy: m=2 r=0.7  Conduction Delay  Number of Peaks  \\\n",
       "36                         0.188661                44                5   \n",
       "38                         0.265399                 9                8   \n",
       "41                         0.147361                13                6   \n",
       "70                         0.060564                11                4   \n",
       "71                         0.150661                 7                4   \n",
       "74                         0.139740                19                7   \n",
       "104                        0.092506                 0                5   \n",
       "107                        0.146783                 1                6   \n",
       "108                        0.156046                30                6   \n",
       "110                        0.236426                 6                7   \n",
       "111                        0.118556                41                5   \n",
       "113                        0.225062                12                6   \n",
       "114                        0.130659                54                5   \n",
       "116                        0.205429                21                6   \n",
       "117                        0.195988                57                8   \n",
       "119                        0.188888                36                6   \n",
       "148                        0.110335                17                3   \n",
       "150                        0.206138                32                6   \n",
       "151                        0.107762                18                5   \n",
       "183                        0.169369                 6                5   \n",
       "189                        0.180150                 5                5   \n",
       "192                        0.181776                 9                8   \n",
       "193                        0.172950                 2                5   \n",
       "194                        0.106949                 0                5   \n",
       "195                        0.197321                12                6   \n",
       "196                        0.178037                 5                5   \n",
       "197                        0.208397                 0                8   \n",
       "198                        0.188709                 0                6   \n",
       "199                        0.215483                29                5   \n",
       "200                        0.205593                22                6   \n",
       "228                        0.178670                36                7   \n",
       "229                        0.134803                33                4   \n",
       "230                        0.107180                25                5   \n",
       "231                        0.263119                35                7   \n",
       "232                        0.159768                33                4   \n",
       "233                        0.139712                24                6   \n",
       "261                        0.176980                19                5   \n",
       "262                        0.074642                11                5   \n",
       "264                        0.206701                20                6   \n",
       "267                        0.154928                19                7   \n",
       "268                        0.125688                12                5   \n",
       "270                        0.240463                22                8   \n",
       "273                        0.243056                16                9   \n",
       "276                        0.197682                27                7   \n",
       "277                        0.153212                21                4   \n",
       "291                        0.087896                 7                5   \n",
       "294                        0.082989                 8                5   \n",
       "418                        0.043526                 9                4   \n",
       "421                        0.068250                12                4   \n",
       "422                        0.040497                 3                4   \n",
       "533                        0.133678                 7                5   \n",
       "594                        0.108492                32                3   \n",
       "595                        0.178303                21                5   \n",
       "596                        0.120622                 0                5   \n",
       "\n",
       "     Percentage Fractionation  \n",
       "36                  13.333333  \n",
       "38                  25.333333  \n",
       "41                  16.666667  \n",
       "70                  10.000000  \n",
       "71                   9.333333  \n",
       "74                  18.666667  \n",
       "104                 16.000000  \n",
       "107                 11.333333  \n",
       "108                 21.333333  \n",
       "110                 15.333333  \n",
       "111                 17.333333  \n",
       "113                 18.666667  \n",
       "114                 18.000000  \n",
       "116                 18.666667  \n",
       "117                 18.666667  \n",
       "119                 12.666667  \n",
       "148                  6.000000  \n",
       "150                 17.333333  \n",
       "151                 13.333333  \n",
       "183                 17.333333  \n",
       "189                  7.333333  \n",
       "192                  5.333333  \n",
       "193                 10.000000  \n",
       "194                 16.666667  \n",
       "195                  6.000000  \n",
       "196                 14.000000  \n",
       "197                 26.000000  \n",
       "198                  0.000000  \n",
       "199                  9.333333  \n",
       "200                 14.000000  \n",
       "228                 12.000000  \n",
       "229                 12.000000  \n",
       "230                  9.333333  \n",
       "231                 23.333333  \n",
       "232                 13.333333  \n",
       "233                 17.333333  \n",
       "261                 18.000000  \n",
       "262                 12.000000  \n",
       "264                  8.666667  \n",
       "267                 14.666667  \n",
       "268                 14.000000  \n",
       "270                 22.666667  \n",
       "273                 20.666667  \n",
       "276                 12.000000  \n",
       "277                  2.666667  \n",
       "291                 10.000000  \n",
       "294                 11.333333  \n",
       "418                  6.666667  \n",
       "421                  6.000000  \n",
       "422                  8.666667  \n",
       "533                 13.333333  \n",
       "594                 10.000000  \n",
       "595                  6.666667  \n",
       "596                 11.333333  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_training[y_training.values == '1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_cm(cm, labels, hide_zeroes=False, hide_diagonal=False, hide_threshold=None):\n",
    "    \"\"\"pretty print for confusion matrixes\"\"\"\n",
    "    columnwidth = max([len(x) for x in labels] + [5])  # 5 is value length\n",
    "    empty_cell = \" \" * columnwidth\n",
    "    \n",
    "    # Begin CHANGES\n",
    "    fst_empty_cell = (columnwidth-3)//2 * \" \" + \"t/p\" + (columnwidth-3)//2 * \" \"\n",
    "    \n",
    "    if len(fst_empty_cell) < len(empty_cell):\n",
    "        fst_empty_cell = \" \" * (len(empty_cell) - len(fst_empty_cell)) + fst_empty_cell\n",
    "    # Print header\n",
    "    print(\"    \" + fst_empty_cell, end=\" \")\n",
    "    # End CHANGES\n",
    "    \n",
    "    for label in labels:\n",
    "        print(\"%{0}s\".format(columnwidth) % label, end=\" \")\n",
    "        \n",
    "    print()\n",
    "    # Print rows\n",
    "    for i, label1 in enumerate(labels):\n",
    "        print(\"    %{0}s\".format(columnwidth) % label1, end=\" \")\n",
    "        for j in range(len(labels)):\n",
    "            cell = \"%{0}.1f\".format(columnwidth) % cm[i, j]\n",
    "            if hide_zeroes:\n",
    "                cell = cell if float(cm[i, j]) != 0 else empty_cell\n",
    "            if hide_diagonal:\n",
    "                cell = cell if i != j else empty_cell\n",
    "            if hide_threshold:\n",
    "                cell = cell if cm[i, j] > hide_threshold else empty_cell\n",
    "            print(cell, end=\" \")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results of Classifiers - Feature Selection Using Feature Importance\n",
    "Here features are selected using feature importance/recursive feature elimination. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of features: 4\n",
      "Number of features selected: 4\n",
      "['Approximate Entropy: m=2 r=0.7', 'Conduction Delay', 'Number of Peaks', 'Percentage Fractionation']\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Logistic Regression\n",
      "Cross validation score:\n",
      "[0.85238095 0.89047619 0.92380952]\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "RBF SVC\n",
      "Cross validation score:\n",
      "[0.85714286 0.87619048 0.86666667]\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Linear SVC\n",
      "Cross validation score:\n",
      "[0.87142857 0.8952381  0.93333333]\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Naive Bayes\n",
      "Cross validation score:\n",
      "[0.92857143 0.94285714 0.93333333]\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "GP\n",
      "Cross validation score:\n",
      "[0.94761905 0.90952381 0.92857143]\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "XGBoost\n",
      "Cross validation score:\n",
      "[0.93809524 0.9        0.91904762]\n"
     ]
    }
   ],
   "source": [
    "# Get cross validation scores on training data, following by test score.\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "models = (LogisticRegression(penalty='l2', C=0.5, random_state=0, solver='liblinear', class_weight=\"balanced\"), \n",
    "          svm.SVC(class_weight=\"balanced\"), \n",
    "          svm.LinearSVC(penalty='l2', C=1, dual=False, class_weight=\"balanced\"),\n",
    "          naive_bayes.GaussianNB(),  \n",
    "          gaussian_process.GaussianProcessClassifier(kernel=1.0*RBF(1)), \n",
    "          xgboost.XGBClassifier(scale_pos_weight=(1/prop_fractionated)))\n",
    "model_names = ('Logistic Regression', 'RBF SVC', 'Linear SVC', 'Naive Bayes', 'GP', 'XGBoost')\n",
    "\n",
    "feature_names = x_training.columns\n",
    "\n",
    "print('Original number of features: ' + str(x_training.shape[1]))\n",
    "\n",
    "# Select according to feature importance. \n",
    "xgb = xgboost.XGBClassifier()\n",
    "linear_svc = svm.LinearSVC(penalty='l2', C=1, dual=False)\n",
    "log_regression = LogisticRegression(penalty='l2', C=1, random_state=0, solver='liblinear')\n",
    "\n",
    "\n",
    "sfm = RFECV(log_regression, step=1, cv=3)\n",
    "sfm.fit(x_training.values, y_training.values)\n",
    "x_training_sparse = sfm.transform(x_training.values)\n",
    "print('Number of features selected: ' + str(x_training_sparse.shape[1]))\n",
    "mask = sfm.get_support() #list of booleans\n",
    "selected_features = [] # The list of your K best features\n",
    "\n",
    "for bool, feature in zip(mask, feature_names):\n",
    "    if bool:\n",
    "        selected_features.append(feature)\n",
    "        \n",
    "print(selected_features)\n",
    "\n",
    "for clf, model_name in zip(models, model_names):\n",
    "    print('~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "    print(model_name)\n",
    "    print('Cross validation score:')\n",
    "    print(cross_val_score(clf, x_training_sparse, y_training.values, cv=3))\n",
    "\n",
    "# print('Test Score:')\n",
    "# X_test_sparse = sfm.transform(X_test.values)\n",
    "# xgb.fit(X_training_sparse, y_training.values)\n",
    "# print(xgb.score(X_test_sparse, y_test.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Logistic Regression\n",
      "Test data score:\n",
      "0.8259587020648967\n",
      "           t/p       Not Fractionated     Fractionated \n",
      "    Not Fractionated            256.0             46.0 \n",
      "        Fractionated             13.0             24.0 \n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "RBF SVC\n",
      "Test data score:\n",
      "0.8672566371681416\n",
      "           t/p       Not Fractionated     Fractionated \n",
      "    Not Fractionated            279.0             23.0 \n",
      "        Fractionated             22.0             15.0 \n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Linear SVC\n",
      "Test data score:\n",
      "0.8407079646017699\n",
      "           t/p       Not Fractionated     Fractionated \n",
      "    Not Fractionated            260.0             42.0 \n",
      "        Fractionated             12.0             25.0 \n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Naive Bayes\n",
      "Test data score:\n",
      "0.8761061946902655\n",
      "           t/p       Not Fractionated     Fractionated \n",
      "    Not Fractionated            276.0             26.0 \n",
      "        Fractionated             16.0             21.0 \n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "GP\n",
      "Test data score:\n",
      "0.9056047197640118\n",
      "           t/p       Not Fractionated     Fractionated \n",
      "    Not Fractionated            293.0              9.0 \n",
      "        Fractionated             23.0             14.0 \n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "XGBoost\n",
      "Test data score:\n",
      "0.8908554572271387\n",
      "           t/p       Not Fractionated     Fractionated \n",
      "    Not Fractionated            280.0             22.0 \n",
      "        Fractionated             15.0             22.0 \n"
     ]
    }
   ],
   "source": [
    "x_test_sparse = sfm.transform(x_test.values)\n",
    "\n",
    "for clf, model_name in zip(models, model_names):\n",
    "    print('~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "    print(model_name)\n",
    "    clf.fit(x_training_sparse, y_training.values)\n",
    "    print('Test data score:')\n",
    "    print(clf.score(x_test_sparse, y_test.values))\n",
    "    \n",
    "    predictions = clf.predict(x_test_sparse)\n",
    "    cm = confusion_matrix(y_test.values, predictions)\n",
    "    print_cm(cm, ['Not Fractionated','Fractionated'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
